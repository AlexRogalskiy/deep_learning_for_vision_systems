{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a toy dataset of only two features and four label classes\n",
    "X, y = make_blobs(n_samples=1000, centers=4, n_features=2, cluster_std=2, random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 1, 1, 0, 1, 2, 3, 2, 1, 0, 1, 0, 0, 2, 2, 3, 2, 1, 3, 0,\n",
       "       0, 3, 1, 3, 3, 3, 0, 2, 0, 0, 2, 3, 1, 0, 1, 2, 2, 1, 3, 3, 1, 2,\n",
       "       3, 0, 0, 1, 1, 0, 0, 3, 0, 3, 2, 3, 3, 0, 2, 1, 3, 1, 3, 0, 1, 3,\n",
       "       0, 1, 3, 0, 2, 1, 3, 3, 2, 2, 1, 1, 3, 2, 3, 1, 2, 2, 0, 2, 3, 3,\n",
       "       1, 2, 3, 0, 2, 0, 1, 2, 2, 0, 3, 0, 3, 3, 3, 1, 1, 2, 2, 1, 0, 3,\n",
       "       0, 3, 1, 3, 0, 0, 0, 0, 2, 2, 0, 3, 0, 2, 1, 2, 3, 0, 1, 0, 2, 0,\n",
       "       0, 0, 1, 0, 3, 2, 1, 1, 3, 1, 0, 2, 3, 3, 1, 1, 1, 3, 1, 1, 3, 0,\n",
       "       1, 1, 1, 3, 2, 2, 0, 2, 1, 2, 0, 2, 0, 1, 1, 2, 1, 3, 3, 0, 3, 2,\n",
       "       1, 2, 2, 3, 3, 0, 2, 2, 2, 1, 2, 1, 0, 0, 1, 3, 2, 2, 1, 0, 3, 2,\n",
       "       3, 3, 3, 3, 3, 2, 3, 1, 2, 3, 0, 2, 0, 2, 3, 3, 0, 3, 0, 3, 0, 3,\n",
       "       2, 3, 3, 0, 2, 1, 1, 1, 3, 3, 2, 1, 3, 3, 3, 3, 2, 0, 2, 3, 1, 1,\n",
       "       3, 2, 0, 0, 2, 0, 2, 1, 0, 3, 1, 1, 1, 0, 1, 2, 3, 1, 2, 1, 2, 2,\n",
       "       2, 0, 2, 0, 3, 2, 1, 0, 1, 1, 0, 2, 0, 2, 1, 0, 3, 2, 3, 1, 3, 0,\n",
       "       2, 0, 2, 3, 2, 1, 3, 1, 2, 1, 2, 0, 3, 0, 3, 0, 2, 1, 1, 2, 0, 1,\n",
       "       1, 0, 2, 3, 3, 2, 2, 0, 0, 2, 2, 1, 1, 2, 1, 0, 1, 3, 2, 1, 1, 1,\n",
       "       2, 1, 1, 3, 3, 3, 0, 2, 1, 1, 2, 1, 0, 1, 2, 0, 0, 3, 2, 0, 2, 1,\n",
       "       1, 0, 2, 1, 2, 1, 0, 0, 1, 3, 3, 0, 1, 2, 2, 0, 0, 0, 3, 2, 0, 1,\n",
       "       0, 3, 3, 2, 3, 0, 2, 0, 3, 3, 3, 1, 2, 0, 3, 1, 2, 3, 3, 0, 0, 2,\n",
       "       0, 3, 3, 1, 3, 1, 0, 2, 3, 1, 1, 1, 1, 0, 0, 1, 3, 2, 2, 3, 0, 2,\n",
       "       2, 0, 2, 2, 2, 3, 0, 0, 0, 3, 1, 1, 3, 2, 1, 2, 2, 1, 1, 0, 3, 2,\n",
       "       0, 3, 1, 2, 2, 0, 1, 0, 0, 3, 0, 0, 1, 0, 1, 3, 1, 1, 1, 0, 3, 2,\n",
       "       2, 2, 2, 0, 0, 3, 2, 2, 0, 3, 0, 0, 2, 3, 0, 1, 1, 0, 3, 1, 0, 3,\n",
       "       2, 0, 1, 0, 1, 0, 2, 0, 1, 2, 2, 1, 1, 0, 2, 3, 1, 1, 1, 1, 1, 0,\n",
       "       0, 2, 1, 3, 1, 0, 3, 0, 0, 3, 2, 1, 1, 0, 3, 2, 2, 1, 3, 2, 1, 1,\n",
       "       3, 2, 3, 1, 3, 2, 3, 2, 2, 3, 1, 2, 0, 3, 2, 2, 0, 0, 3, 3, 2, 2,\n",
       "       2, 1, 3, 0, 2, 2, 3, 2, 3, 2, 3, 0, 3, 1, 0, 1, 1, 3, 3, 3, 2, 3,\n",
       "       3, 1, 0, 2, 3, 3, 0, 0, 0, 1, 3, 0, 3, 0, 0, 3, 1, 1, 3, 3, 2, 1,\n",
       "       2, 3, 3, 3, 1, 3, 3, 2, 0, 2, 1, 0, 1, 3, 0, 2, 2, 2, 3, 3, 0, 0,\n",
       "       3, 0, 2, 2, 0, 0, 0, 3, 3, 1, 1, 3, 3, 1, 0, 3, 0, 1, 1, 0, 2, 3,\n",
       "       0, 3, 1, 3, 0, 2, 1, 3, 3, 3, 0, 1, 3, 0, 0, 2, 3, 2, 0, 3, 1, 2,\n",
       "       3, 0, 3, 3, 0, 1, 2, 1, 2, 2, 2, 1, 3, 2, 2, 1, 0, 3, 2, 3, 2, 1,\n",
       "       1, 2, 0, 3, 3, 3, 1, 1, 1, 2, 1, 0, 3, 2, 0, 1, 2, 1, 0, 0, 1, 1,\n",
       "       1, 0, 3, 3, 0, 1, 1, 1, 2, 0, 3, 2, 1, 2, 1, 2, 0, 1, 1, 1, 0, 2,\n",
       "       2, 3, 1, 2, 1, 3, 2, 3, 0, 0, 0, 0, 0, 2, 2, 0, 1, 0, 0, 1, 0, 3,\n",
       "       0, 0, 2, 0, 3, 1, 0, 3, 1, 2, 1, 1, 0, 1, 2, 2, 1, 1, 3, 1, 2, 1,\n",
       "       0, 0, 3, 2, 1, 0, 1, 0, 3, 0, 3, 0, 0, 3, 1, 1, 3, 2, 3, 1, 2, 3,\n",
       "       1, 2, 3, 2, 2, 3, 1, 1, 1, 3, 0, 0, 2, 2, 1, 1, 0, 3, 2, 0, 0, 1,\n",
       "       3, 2, 2, 0, 3, 3, 3, 0, 3, 2, 2, 2, 0, 2, 2, 1, 1, 3, 2, 0, 0, 3,\n",
       "       2, 2, 3, 0, 2, 0, 1, 3, 2, 1, 1, 1, 2, 3, 0, 3, 3, 2, 0, 2, 1, 3,\n",
       "       0, 3, 2, 0, 1, 0, 1, 2, 2, 1, 1, 2, 0, 1, 0, 2, 1, 3, 2, 2, 2, 3,\n",
       "       3, 0, 0, 1, 3, 1, 1, 1, 2, 2, 2, 1, 0, 0, 3, 2, 2, 3, 3, 1, 0, 2,\n",
       "       1, 3, 3, 2, 0, 0, 2, 3, 1, 0, 2, 3, 0, 1, 2, 3, 0, 3, 1, 2, 2, 2,\n",
       "       0, 0, 1, 0, 0, 3, 2, 2, 0, 3, 1, 3, 2, 0, 0, 1, 2, 3, 3, 2, 1, 3,\n",
       "       0, 1, 1, 0, 0, 1, 3, 2, 1, 1, 3, 3, 2, 0, 3, 0, 3, 2, 2, 0, 3, 3,\n",
       "       1, 0, 0, 2, 1, 3, 3, 0, 2, 3, 1, 0, 1, 1, 2, 0, 3, 2, 0, 1, 0, 0,\n",
       "       3, 0, 1, 0, 3, 1, 1, 2, 0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's take a look at the label (y) before one-hot encoding\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode output variable\n",
    "y = to_categorical(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2) (800, 2)\n"
     ]
    }
   ],
   "source": [
    "# split into 80% training data and 20% test data \n",
    "# note that we did not create a validation dataset in this example for simplicity\n",
    "n_train = 200\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 25)                75        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 104       \n",
      "=================================================================\n",
      "Total params: 179\n",
      "Trainable params: 179\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# develop the baseline model architecture\n",
    "# here we are building a very simple, two-layer network\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax')) # four hidden units because we have 4 label classes\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 800 samples\n",
      "Epoch 1/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.3162 - acc: 0.3500 - val_loss: 1.2489 - val_acc: 0.3312\n",
      "Epoch 2/500\n",
      "200/200 [==============================] - 0s 211us/step - loss: 1.2273 - acc: 0.4150 - val_loss: 1.1631 - val_acc: 0.5225\n",
      "Epoch 3/500\n",
      "200/200 [==============================] - 0s 202us/step - loss: 1.1496 - acc: 0.5700 - val_loss: 1.0873 - val_acc: 0.5737\n",
      "Epoch 4/500\n",
      "200/200 [==============================] - 0s 171us/step - loss: 1.0774 - acc: 0.6150 - val_loss: 1.0222 - val_acc: 0.5863\n",
      "Epoch 5/500\n",
      "200/200 [==============================] - 0s 259us/step - loss: 1.0150 - acc: 0.6200 - val_loss: 0.9675 - val_acc: 0.5938\n",
      "Epoch 6/500\n",
      "200/200 [==============================] - 0s 213us/step - loss: 0.9657 - acc: 0.6200 - val_loss: 0.9207 - val_acc: 0.5988\n",
      "Epoch 7/500\n",
      "200/200 [==============================] - 0s 170us/step - loss: 0.9221 - acc: 0.6250 - val_loss: 0.8809 - val_acc: 0.6250\n",
      "Epoch 8/500\n",
      "200/200 [==============================] - 0s 183us/step - loss: 0.8815 - acc: 0.6550 - val_loss: 0.8492 - val_acc: 0.6400\n",
      "Epoch 9/500\n",
      "200/200 [==============================] - 0s 214us/step - loss: 0.8495 - acc: 0.6500 - val_loss: 0.8212 - val_acc: 0.6550\n",
      "Epoch 10/500\n",
      "200/200 [==============================] - 0s 175us/step - loss: 0.8202 - acc: 0.6550 - val_loss: 0.7980 - val_acc: 0.6650\n",
      "Epoch 11/500\n",
      "200/200 [==============================] - 0s 208us/step - loss: 0.7946 - acc: 0.6650 - val_loss: 0.7785 - val_acc: 0.6750\n",
      "Epoch 12/500\n",
      "200/200 [==============================] - 0s 262us/step - loss: 0.7731 - acc: 0.6700 - val_loss: 0.7600 - val_acc: 0.6800\n",
      "Epoch 13/500\n",
      "200/200 [==============================] - 0s 172us/step - loss: 0.7522 - acc: 0.6900 - val_loss: 0.7440 - val_acc: 0.6887\n",
      "Epoch 14/500\n",
      "200/200 [==============================] - 0s 228us/step - loss: 0.7355 - acc: 0.7100 - val_loss: 0.7306 - val_acc: 0.6925\n",
      "Epoch 15/500\n",
      "200/200 [==============================] - 0s 221us/step - loss: 0.7197 - acc: 0.7150 - val_loss: 0.7192 - val_acc: 0.6950\n",
      "Epoch 16/500\n",
      "200/200 [==============================] - 0s 179us/step - loss: 0.7074 - acc: 0.7300 - val_loss: 0.7089 - val_acc: 0.7037\n",
      "Epoch 17/500\n",
      "200/200 [==============================] - 0s 222us/step - loss: 0.6944 - acc: 0.7350 - val_loss: 0.7002 - val_acc: 0.7087\n",
      "Epoch 18/500\n",
      "200/200 [==============================] - 0s 176us/step - loss: 0.6862 - acc: 0.7400 - val_loss: 0.6944 - val_acc: 0.7063\n",
      "Epoch 19/500\n",
      "200/200 [==============================] - 0s 164us/step - loss: 0.6757 - acc: 0.7400 - val_loss: 0.6855 - val_acc: 0.7137\n",
      "Epoch 20/500\n",
      "200/200 [==============================] - 0s 210us/step - loss: 0.6663 - acc: 0.7400 - val_loss: 0.6780 - val_acc: 0.7200\n",
      "Epoch 21/500\n",
      "200/200 [==============================] - 0s 239us/step - loss: 0.6582 - acc: 0.7450 - val_loss: 0.6725 - val_acc: 0.7225\n",
      "Epoch 22/500\n",
      "200/200 [==============================] - 0s 199us/step - loss: 0.6511 - acc: 0.7500 - val_loss: 0.6677 - val_acc: 0.7250\n",
      "Epoch 23/500\n",
      "200/200 [==============================] - 0s 172us/step - loss: 0.6437 - acc: 0.7550 - val_loss: 0.6618 - val_acc: 0.7275\n",
      "Epoch 24/500\n",
      "200/200 [==============================] - 0s 164us/step - loss: 0.6367 - acc: 0.7550 - val_loss: 0.6568 - val_acc: 0.7288\n",
      "Epoch 25/500\n",
      "200/200 [==============================] - 0s 233us/step - loss: 0.6305 - acc: 0.7600 - val_loss: 0.6521 - val_acc: 0.7350\n",
      "Epoch 26/500\n",
      "200/200 [==============================] - 0s 167us/step - loss: 0.6253 - acc: 0.7700 - val_loss: 0.6476 - val_acc: 0.7438\n",
      "Epoch 27/500\n",
      "200/200 [==============================] - 0s 179us/step - loss: 0.6197 - acc: 0.7750 - val_loss: 0.6433 - val_acc: 0.7475\n",
      "Epoch 28/500\n",
      "200/200 [==============================] - 0s 191us/step - loss: 0.6132 - acc: 0.7800 - val_loss: 0.6394 - val_acc: 0.7425\n",
      "Epoch 29/500\n",
      "200/200 [==============================] - 0s 190us/step - loss: 0.6071 - acc: 0.7800 - val_loss: 0.6364 - val_acc: 0.7400\n",
      "Epoch 30/500\n",
      "200/200 [==============================] - 0s 207us/step - loss: 0.6031 - acc: 0.7750 - val_loss: 0.6332 - val_acc: 0.7425\n",
      "Epoch 31/500\n",
      "200/200 [==============================] - 0s 169us/step - loss: 0.5987 - acc: 0.7700 - val_loss: 0.6287 - val_acc: 0.7425\n",
      "Epoch 32/500\n",
      "200/200 [==============================] - 0s 191us/step - loss: 0.5943 - acc: 0.7750 - val_loss: 0.6245 - val_acc: 0.7462\n",
      "Epoch 33/500\n",
      "200/200 [==============================] - 0s 198us/step - loss: 0.5896 - acc: 0.7800 - val_loss: 0.6196 - val_acc: 0.7512\n",
      "Epoch 34/500\n",
      "200/200 [==============================] - 0s 179us/step - loss: 0.5852 - acc: 0.7850 - val_loss: 0.6156 - val_acc: 0.7512\n",
      "Epoch 35/500\n",
      "200/200 [==============================] - 0s 232us/step - loss: 0.5811 - acc: 0.7850 - val_loss: 0.6111 - val_acc: 0.7512\n",
      "Epoch 36/500\n",
      "200/200 [==============================] - 0s 162us/step - loss: 0.5772 - acc: 0.7850 - val_loss: 0.6071 - val_acc: 0.7600\n",
      "Epoch 37/500\n",
      "200/200 [==============================] - 0s 168us/step - loss: 0.5747 - acc: 0.7850 - val_loss: 0.6044 - val_acc: 0.7600\n",
      "Epoch 38/500\n",
      "200/200 [==============================] - 0s 228us/step - loss: 0.5705 - acc: 0.7900 - val_loss: 0.6021 - val_acc: 0.7562\n",
      "Epoch 39/500\n",
      "200/200 [==============================] - 0s 168us/step - loss: 0.5666 - acc: 0.7850 - val_loss: 0.5999 - val_acc: 0.7575\n",
      "Epoch 40/500\n",
      "200/200 [==============================] - 0s 249us/step - loss: 0.5632 - acc: 0.7850 - val_loss: 0.5972 - val_acc: 0.7612\n",
      "Epoch 41/500\n",
      "200/200 [==============================] - 0s 183us/step - loss: 0.5594 - acc: 0.7850 - val_loss: 0.5949 - val_acc: 0.7575\n",
      "Epoch 42/500\n",
      "200/200 [==============================] - 0s 179us/step - loss: 0.5558 - acc: 0.7850 - val_loss: 0.5923 - val_acc: 0.7575\n",
      "Epoch 43/500\n",
      "200/200 [==============================] - 0s 164us/step - loss: 0.5525 - acc: 0.7900 - val_loss: 0.5900 - val_acc: 0.7600\n",
      "Epoch 44/500\n",
      "200/200 [==============================] - 0s 196us/step - loss: 0.5503 - acc: 0.7900 - val_loss: 0.5873 - val_acc: 0.7612\n",
      "Epoch 45/500\n",
      "200/200 [==============================] - 0s 163us/step - loss: 0.5467 - acc: 0.7900 - val_loss: 0.5846 - val_acc: 0.7612\n",
      "Epoch 46/500\n",
      "200/200 [==============================] - 0s 172us/step - loss: 0.5440 - acc: 0.7900 - val_loss: 0.5821 - val_acc: 0.7612\n",
      "Epoch 47/500\n",
      "200/200 [==============================] - 0s 161us/step - loss: 0.5408 - acc: 0.7950 - val_loss: 0.5803 - val_acc: 0.7612\n",
      "Epoch 48/500\n",
      "200/200 [==============================] - 0s 177us/step - loss: 0.5386 - acc: 0.7950 - val_loss: 0.5776 - val_acc: 0.7650\n",
      "Epoch 49/500\n",
      "200/200 [==============================] - 0s 199us/step - loss: 0.5349 - acc: 0.7950 - val_loss: 0.5761 - val_acc: 0.7600\n",
      "Epoch 50/500\n",
      "200/200 [==============================] - 0s 227us/step - loss: 0.5331 - acc: 0.8000 - val_loss: 0.5751 - val_acc: 0.7588\n",
      "Epoch 51/500\n",
      "200/200 [==============================] - 0s 241us/step - loss: 0.5287 - acc: 0.8150 - val_loss: 0.5719 - val_acc: 0.7688\n",
      "Epoch 52/500\n",
      "200/200 [==============================] - 0s 249us/step - loss: 0.5261 - acc: 0.8100 - val_loss: 0.5689 - val_acc: 0.7700\n",
      "Epoch 53/500\n",
      "200/200 [==============================] - 0s 188us/step - loss: 0.5244 - acc: 0.8250 - val_loss: 0.5679 - val_acc: 0.7662\n",
      "Epoch 54/500\n",
      "200/200 [==============================] - 0s 276us/step - loss: 0.5228 - acc: 0.8100 - val_loss: 0.5666 - val_acc: 0.7662\n",
      "Epoch 55/500\n",
      "200/200 [==============================] - 0s 216us/step - loss: 0.5192 - acc: 0.8150 - val_loss: 0.5637 - val_acc: 0.7712\n",
      "Epoch 56/500\n",
      "200/200 [==============================] - 0s 207us/step - loss: 0.5167 - acc: 0.8150 - val_loss: 0.5605 - val_acc: 0.7688\n",
      "Epoch 57/500\n",
      "200/200 [==============================] - 0s 233us/step - loss: 0.5136 - acc: 0.8200 - val_loss: 0.5571 - val_acc: 0.7688\n",
      "Epoch 58/500\n",
      "200/200 [==============================] - 0s 164us/step - loss: 0.5111 - acc: 0.8300 - val_loss: 0.5538 - val_acc: 0.7675\n",
      "Epoch 59/500\n",
      "200/200 [==============================] - 0s 191us/step - loss: 0.5087 - acc: 0.8350 - val_loss: 0.5493 - val_acc: 0.7762\n",
      "Epoch 60/500\n",
      "200/200 [==============================] - 0s 192us/step - loss: 0.5062 - acc: 0.8350 - val_loss: 0.5471 - val_acc: 0.7775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "200/200 [==============================] - 0s 182us/step - loss: 0.5048 - acc: 0.8350 - val_loss: 0.5453 - val_acc: 0.7812\n",
      "Epoch 62/500\n",
      "200/200 [==============================] - 0s 163us/step - loss: 0.5028 - acc: 0.8300 - val_loss: 0.5448 - val_acc: 0.7837\n",
      "Epoch 63/500\n",
      "200/200 [==============================] - 0s 193us/step - loss: 0.5004 - acc: 0.8350 - val_loss: 0.5435 - val_acc: 0.7825\n",
      "Epoch 64/500\n",
      "200/200 [==============================] - 0s 179us/step - loss: 0.4972 - acc: 0.8350 - val_loss: 0.5423 - val_acc: 0.7788\n",
      "Epoch 65/500\n",
      "200/200 [==============================] - 0s 184us/step - loss: 0.4953 - acc: 0.8400 - val_loss: 0.5403 - val_acc: 0.7837\n",
      "Epoch 66/500\n",
      "200/200 [==============================] - 0s 195us/step - loss: 0.4931 - acc: 0.8400 - val_loss: 0.5388 - val_acc: 0.7812\n",
      "Epoch 67/500\n",
      "200/200 [==============================] - 0s 165us/step - loss: 0.4908 - acc: 0.8400 - val_loss: 0.5371 - val_acc: 0.7837\n",
      "Epoch 68/500\n",
      "200/200 [==============================] - 0s 186us/step - loss: 0.4893 - acc: 0.8350 - val_loss: 0.5355 - val_acc: 0.7825\n",
      "Epoch 69/500\n",
      "200/200 [==============================] - 0s 264us/step - loss: 0.4869 - acc: 0.8450 - val_loss: 0.5342 - val_acc: 0.7800\n",
      "Epoch 70/500\n",
      "200/200 [==============================] - 0s 224us/step - loss: 0.4849 - acc: 0.8400 - val_loss: 0.5324 - val_acc: 0.7800\n",
      "Epoch 71/500\n",
      "200/200 [==============================] - 0s 276us/step - loss: 0.4828 - acc: 0.8400 - val_loss: 0.5306 - val_acc: 0.7837\n",
      "Epoch 72/500\n",
      "200/200 [==============================] - 0s 211us/step - loss: 0.4818 - acc: 0.8450 - val_loss: 0.5300 - val_acc: 0.7825\n",
      "Epoch 73/500\n",
      "200/200 [==============================] - 0s 240us/step - loss: 0.4796 - acc: 0.8450 - val_loss: 0.5269 - val_acc: 0.7863\n",
      "Epoch 74/500\n",
      "200/200 [==============================] - 0s 200us/step - loss: 0.4770 - acc: 0.8450 - val_loss: 0.5232 - val_acc: 0.7937\n",
      "Epoch 75/500\n",
      "200/200 [==============================] - 0s 168us/step - loss: 0.4754 - acc: 0.8350 - val_loss: 0.5211 - val_acc: 0.7975\n",
      "Epoch 76/500\n",
      "200/200 [==============================] - 0s 180us/step - loss: 0.4756 - acc: 0.8450 - val_loss: 0.5176 - val_acc: 0.8000\n",
      "Epoch 77/500\n",
      "200/200 [==============================] - 0s 145us/step - loss: 0.4729 - acc: 0.8450 - val_loss: 0.5151 - val_acc: 0.7963\n",
      "Epoch 78/500\n",
      "200/200 [==============================] - 0s 172us/step - loss: 0.4702 - acc: 0.8450 - val_loss: 0.5150 - val_acc: 0.7950\n",
      "Epoch 79/500\n",
      "200/200 [==============================] - 0s 155us/step - loss: 0.4688 - acc: 0.8550 - val_loss: 0.5148 - val_acc: 0.7937\n",
      "Epoch 80/500\n",
      "200/200 [==============================] - 0s 223us/step - loss: 0.4662 - acc: 0.8450 - val_loss: 0.5133 - val_acc: 0.7963\n",
      "Epoch 81/500\n",
      "200/200 [==============================] - 0s 170us/step - loss: 0.4644 - acc: 0.8450 - val_loss: 0.5114 - val_acc: 0.7975\n",
      "Epoch 82/500\n",
      "200/200 [==============================] - 0s 191us/step - loss: 0.4628 - acc: 0.8500 - val_loss: 0.5094 - val_acc: 0.8000\n",
      "Epoch 83/500\n",
      "200/200 [==============================] - 0s 202us/step - loss: 0.4609 - acc: 0.8550 - val_loss: 0.5081 - val_acc: 0.7975\n",
      "Epoch 84/500\n",
      "200/200 [==============================] - 0s 180us/step - loss: 0.4596 - acc: 0.8550 - val_loss: 0.5067 - val_acc: 0.7963\n",
      "Epoch 85/500\n",
      "200/200 [==============================] - 0s 175us/step - loss: 0.4575 - acc: 0.8500 - val_loss: 0.5056 - val_acc: 0.7937\n",
      "Epoch 86/500\n",
      "200/200 [==============================] - 0s 175us/step - loss: 0.4557 - acc: 0.8600 - val_loss: 0.5045 - val_acc: 0.7937\n",
      "Epoch 87/500\n",
      "200/200 [==============================] - 0s 166us/step - loss: 0.4546 - acc: 0.8600 - val_loss: 0.5024 - val_acc: 0.7950\n",
      "Epoch 88/500\n",
      "200/200 [==============================] - 0s 180us/step - loss: 0.4526 - acc: 0.8500 - val_loss: 0.5014 - val_acc: 0.7950\n",
      "Epoch 89/500\n",
      "200/200 [==============================] - 0s 200us/step - loss: 0.4509 - acc: 0.8600 - val_loss: 0.5003 - val_acc: 0.7975\n",
      "Epoch 90/500\n",
      "200/200 [==============================] - 0s 180us/step - loss: 0.4494 - acc: 0.8550 - val_loss: 0.4996 - val_acc: 0.7963\n",
      "Epoch 91/500\n",
      "200/200 [==============================] - 0s 150us/step - loss: 0.4476 - acc: 0.8550 - val_loss: 0.4982 - val_acc: 0.8037\n",
      "Epoch 92/500\n",
      "200/200 [==============================] - 0s 141us/step - loss: 0.4457 - acc: 0.8500 - val_loss: 0.4964 - val_acc: 0.8025\n",
      "Epoch 93/500\n",
      "200/200 [==============================] - 0s 203us/step - loss: 0.4443 - acc: 0.8500 - val_loss: 0.4950 - val_acc: 0.8025\n",
      "Epoch 94/500\n",
      "200/200 [==============================] - 0s 164us/step - loss: 0.4424 - acc: 0.8550 - val_loss: 0.4924 - val_acc: 0.8000\n",
      "Epoch 95/500\n",
      "200/200 [==============================] - 0s 176us/step - loss: 0.4424 - acc: 0.8450 - val_loss: 0.4903 - val_acc: 0.8000\n",
      "Epoch 96/500\n",
      "200/200 [==============================] - 0s 205us/step - loss: 0.4401 - acc: 0.8550 - val_loss: 0.4887 - val_acc: 0.8000\n",
      "Epoch 97/500\n",
      "200/200 [==============================] - 0s 169us/step - loss: 0.4386 - acc: 0.8600 - val_loss: 0.4874 - val_acc: 0.8013\n",
      "Epoch 98/500\n",
      "200/200 [==============================] - 0s 184us/step - loss: 0.4366 - acc: 0.8600 - val_loss: 0.4859 - val_acc: 0.8013\n",
      "Epoch 99/500\n",
      "200/200 [==============================] - 0s 196us/step - loss: 0.4354 - acc: 0.8600 - val_loss: 0.4847 - val_acc: 0.8000\n",
      "Epoch 100/500\n",
      "200/200 [==============================] - 0s 172us/step - loss: 0.4344 - acc: 0.8550 - val_loss: 0.4844 - val_acc: 0.8013\n",
      "Epoch 101/500\n",
      "200/200 [==============================] - 0s 207us/step - loss: 0.4326 - acc: 0.8600 - val_loss: 0.4843 - val_acc: 0.8050\n",
      "Epoch 102/500\n",
      "200/200 [==============================] - 0s 158us/step - loss: 0.4310 - acc: 0.8500 - val_loss: 0.4829 - val_acc: 0.8013\n",
      "Epoch 103/500\n",
      "200/200 [==============================] - 0s 174us/step - loss: 0.4298 - acc: 0.8550 - val_loss: 0.4818 - val_acc: 0.8037\n",
      "Epoch 104/500\n",
      "200/200 [==============================] - 0s 211us/step - loss: 0.4294 - acc: 0.8600 - val_loss: 0.4801 - val_acc: 0.8037\n",
      "Epoch 105/500\n",
      "200/200 [==============================] - 0s 139us/step - loss: 0.4275 - acc: 0.8600 - val_loss: 0.4774 - val_acc: 0.8025\n",
      "Epoch 106/500\n",
      "200/200 [==============================] - 0s 176us/step - loss: 0.4259 - acc: 0.8600 - val_loss: 0.4754 - val_acc: 0.8037\n",
      "Epoch 107/500\n",
      "200/200 [==============================] - 0s 151us/step - loss: 0.4247 - acc: 0.8700 - val_loss: 0.4736 - val_acc: 0.8087\n",
      "Epoch 108/500\n",
      "200/200 [==============================] - 0s 191us/step - loss: 0.4233 - acc: 0.8700 - val_loss: 0.4722 - val_acc: 0.8075\n",
      "Epoch 109/500\n",
      "200/200 [==============================] - 0s 164us/step - loss: 0.4220 - acc: 0.8700 - val_loss: 0.4714 - val_acc: 0.8087\n",
      "Epoch 110/500\n",
      "200/200 [==============================] - 0s 141us/step - loss: 0.4209 - acc: 0.8700 - val_loss: 0.4711 - val_acc: 0.8087\n",
      "Epoch 111/500\n",
      "200/200 [==============================] - 0s 176us/step - loss: 0.4195 - acc: 0.8650 - val_loss: 0.4696 - val_acc: 0.8100\n",
      "Epoch 112/500\n",
      "200/200 [==============================] - 0s 215us/step - loss: 0.4179 - acc: 0.8650 - val_loss: 0.4690 - val_acc: 0.8113\n",
      "Epoch 113/500\n",
      "200/200 [==============================] - 0s 150us/step - loss: 0.4177 - acc: 0.8650 - val_loss: 0.4677 - val_acc: 0.8125\n",
      "Epoch 114/500\n",
      "200/200 [==============================] - 0s 194us/step - loss: 0.4167 - acc: 0.8650 - val_loss: 0.4678 - val_acc: 0.8113\n",
      "Epoch 115/500\n",
      "200/200 [==============================] - 0s 146us/step - loss: 0.4140 - acc: 0.8650 - val_loss: 0.4676 - val_acc: 0.8087\n",
      "Epoch 116/500\n",
      "200/200 [==============================] - 0s 145us/step - loss: 0.4130 - acc: 0.8650 - val_loss: 0.4674 - val_acc: 0.8087\n",
      "Epoch 117/500\n",
      "200/200 [==============================] - 0s 162us/step - loss: 0.4124 - acc: 0.8650 - val_loss: 0.4667 - val_acc: 0.8075\n",
      "Epoch 118/500\n",
      "200/200 [==============================] - 0s 170us/step - loss: 0.4107 - acc: 0.8650 - val_loss: 0.4665 - val_acc: 0.8075\n",
      "Epoch 119/500\n",
      "200/200 [==============================] - 0s 150us/step - loss: 0.4096 - acc: 0.8650 - val_loss: 0.4662 - val_acc: 0.8125\n",
      "Epoch 120/500\n",
      "200/200 [==============================] - 0s 133us/step - loss: 0.4090 - acc: 0.8650 - val_loss: 0.4654 - val_acc: 0.8150\n",
      "Epoch 121/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 120us/step - loss: 0.4077 - acc: 0.8650 - val_loss: 0.4628 - val_acc: 0.8137\n",
      "Epoch 122/500\n",
      "200/200 [==============================] - 0s 112us/step - loss: 0.4058 - acc: 0.8650 - val_loss: 0.4600 - val_acc: 0.8125\n",
      "Epoch 123/500\n",
      "200/200 [==============================] - 0s 173us/step - loss: 0.4051 - acc: 0.8700 - val_loss: 0.4575 - val_acc: 0.8137\n",
      "Epoch 124/500\n",
      "200/200 [==============================] - 0s 135us/step - loss: 0.4041 - acc: 0.8700 - val_loss: 0.4557 - val_acc: 0.8150\n",
      "Epoch 125/500\n",
      "200/200 [==============================] - 0s 168us/step - loss: 0.4030 - acc: 0.8650 - val_loss: 0.4544 - val_acc: 0.8150\n",
      "Epoch 126/500\n",
      "200/200 [==============================] - 0s 176us/step - loss: 0.4018 - acc: 0.8700 - val_loss: 0.4541 - val_acc: 0.8150\n",
      "Epoch 127/500\n",
      "200/200 [==============================] - 0s 183us/step - loss: 0.4009 - acc: 0.8700 - val_loss: 0.4544 - val_acc: 0.8137\n",
      "Epoch 128/500\n",
      "200/200 [==============================] - 0s 156us/step - loss: 0.3995 - acc: 0.8700 - val_loss: 0.4532 - val_acc: 0.8113\n",
      "Epoch 129/500\n",
      "200/200 [==============================] - 0s 196us/step - loss: 0.3988 - acc: 0.8650 - val_loss: 0.4513 - val_acc: 0.8137\n",
      "Epoch 130/500\n",
      "200/200 [==============================] - 0s 182us/step - loss: 0.3975 - acc: 0.8700 - val_loss: 0.4506 - val_acc: 0.8163\n",
      "Epoch 131/500\n",
      "200/200 [==============================] - 0s 207us/step - loss: 0.3964 - acc: 0.8750 - val_loss: 0.4501 - val_acc: 0.8187\n",
      "Epoch 132/500\n",
      "200/200 [==============================] - 0s 244us/step - loss: 0.3954 - acc: 0.8750 - val_loss: 0.4496 - val_acc: 0.8200\n",
      "Epoch 133/500\n",
      "200/200 [==============================] - 0s 182us/step - loss: 0.3943 - acc: 0.8750 - val_loss: 0.4484 - val_acc: 0.8213\n",
      "Epoch 134/500\n",
      "200/200 [==============================] - 0s 179us/step - loss: 0.3929 - acc: 0.8750 - val_loss: 0.4482 - val_acc: 0.8175\n",
      "Epoch 135/500\n",
      "200/200 [==============================] - 0s 168us/step - loss: 0.3919 - acc: 0.8750 - val_loss: 0.4481 - val_acc: 0.8175\n",
      "Epoch 136/500\n",
      "200/200 [==============================] - 0s 244us/step - loss: 0.3911 - acc: 0.8750 - val_loss: 0.4466 - val_acc: 0.8163\n",
      "Epoch 137/500\n",
      "200/200 [==============================] - 0s 163us/step - loss: 0.3901 - acc: 0.8750 - val_loss: 0.4470 - val_acc: 0.8175\n",
      "Epoch 138/500\n",
      "200/200 [==============================] - 0s 216us/step - loss: 0.3893 - acc: 0.8750 - val_loss: 0.4469 - val_acc: 0.8163\n",
      "Epoch 139/500\n",
      "200/200 [==============================] - 0s 162us/step - loss: 0.3888 - acc: 0.8750 - val_loss: 0.4454 - val_acc: 0.8163\n",
      "Epoch 140/500\n",
      "200/200 [==============================] - 0s 156us/step - loss: 0.3870 - acc: 0.8750 - val_loss: 0.4452 - val_acc: 0.8163\n",
      "Epoch 141/500\n",
      "200/200 [==============================] - 0s 181us/step - loss: 0.3867 - acc: 0.8700 - val_loss: 0.4464 - val_acc: 0.8150\n",
      "Epoch 142/500\n",
      "200/200 [==============================] - 0s 174us/step - loss: 0.3855 - acc: 0.8700 - val_loss: 0.4457 - val_acc: 0.8163\n",
      "Epoch 143/500\n",
      "200/200 [==============================] - 0s 168us/step - loss: 0.3849 - acc: 0.8700 - val_loss: 0.4460 - val_acc: 0.8163\n",
      "Epoch 144/500\n",
      "200/200 [==============================] - 0s 186us/step - loss: 0.3838 - acc: 0.8700 - val_loss: 0.4461 - val_acc: 0.8150\n",
      "Epoch 145/500\n",
      "200/200 [==============================] - 0s 178us/step - loss: 0.3829 - acc: 0.8750 - val_loss: 0.4444 - val_acc: 0.8175\n",
      "Epoch 146/500\n",
      "200/200 [==============================] - 0s 154us/step - loss: 0.3823 - acc: 0.8750 - val_loss: 0.4420 - val_acc: 0.8163\n",
      "Epoch 147/500\n",
      "200/200 [==============================] - 0s 171us/step - loss: 0.3810 - acc: 0.8800 - val_loss: 0.4390 - val_acc: 0.8200\n",
      "Epoch 148/500\n",
      "200/200 [==============================] - 0s 149us/step - loss: 0.3797 - acc: 0.8800 - val_loss: 0.4372 - val_acc: 0.8213\n",
      "Epoch 149/500\n",
      "200/200 [==============================] - 0s 213us/step - loss: 0.3809 - acc: 0.8800 - val_loss: 0.4368 - val_acc: 0.8187\n",
      "Epoch 150/500\n",
      "200/200 [==============================] - 0s 161us/step - loss: 0.3795 - acc: 0.8750 - val_loss: 0.4364 - val_acc: 0.8200\n",
      "Epoch 151/500\n",
      "200/200 [==============================] - 0s 167us/step - loss: 0.3776 - acc: 0.8800 - val_loss: 0.4363 - val_acc: 0.8175\n",
      "Epoch 152/500\n",
      "200/200 [==============================] - 0s 166us/step - loss: 0.3765 - acc: 0.8800 - val_loss: 0.4367 - val_acc: 0.8175\n",
      "Epoch 153/500\n",
      "200/200 [==============================] - 0s 190us/step - loss: 0.3759 - acc: 0.8800 - val_loss: 0.4355 - val_acc: 0.8200\n",
      "Epoch 154/500\n",
      "200/200 [==============================] - 0s 172us/step - loss: 0.3755 - acc: 0.8850 - val_loss: 0.4326 - val_acc: 0.8237\n",
      "Epoch 155/500\n",
      "200/200 [==============================] - 0s 176us/step - loss: 0.3746 - acc: 0.8850 - val_loss: 0.4325 - val_acc: 0.8250\n",
      "Epoch 156/500\n",
      "200/200 [==============================] - 0s 150us/step - loss: 0.3742 - acc: 0.8800 - val_loss: 0.4321 - val_acc: 0.8237\n",
      "Epoch 157/500\n",
      "200/200 [==============================] - 0s 177us/step - loss: 0.3726 - acc: 0.8800 - val_loss: 0.4315 - val_acc: 0.8225\n",
      "Epoch 158/500\n",
      "200/200 [==============================] - 0s 154us/step - loss: 0.3722 - acc: 0.8850 - val_loss: 0.4313 - val_acc: 0.8200\n",
      "Epoch 159/500\n",
      "200/200 [==============================] - 0s 192us/step - loss: 0.3713 - acc: 0.8850 - val_loss: 0.4317 - val_acc: 0.8187\n",
      "Epoch 160/500\n",
      "200/200 [==============================] - 0s 177us/step - loss: 0.3705 - acc: 0.8800 - val_loss: 0.4308 - val_acc: 0.8200\n",
      "Epoch 161/500\n",
      "200/200 [==============================] - 0s 177us/step - loss: 0.3703 - acc: 0.8800 - val_loss: 0.4296 - val_acc: 0.8213\n",
      "Epoch 162/500\n",
      "200/200 [==============================] - 0s 181us/step - loss: 0.3696 - acc: 0.8750 - val_loss: 0.4285 - val_acc: 0.8213\n",
      "Epoch 163/500\n",
      "200/200 [==============================] - 0s 162us/step - loss: 0.3687 - acc: 0.8750 - val_loss: 0.4276 - val_acc: 0.8225\n",
      "Epoch 164/500\n",
      "200/200 [==============================] - 0s 220us/step - loss: 0.3682 - acc: 0.8800 - val_loss: 0.4267 - val_acc: 0.8200\n",
      "Epoch 165/500\n",
      "200/200 [==============================] - 0s 186us/step - loss: 0.3677 - acc: 0.8850 - val_loss: 0.4268 - val_acc: 0.8175\n",
      "Epoch 166/500\n",
      "200/200 [==============================] - 0s 131us/step - loss: 0.3667 - acc: 0.8850 - val_loss: 0.4264 - val_acc: 0.8225\n",
      "Epoch 167/500\n",
      "200/200 [==============================] - 0s 176us/step - loss: 0.3658 - acc: 0.8800 - val_loss: 0.4261 - val_acc: 0.8250\n",
      "Epoch 168/500\n",
      "200/200 [==============================] - 0s 160us/step - loss: 0.3655 - acc: 0.8800 - val_loss: 0.4264 - val_acc: 0.8213\n",
      "Epoch 169/500\n",
      "200/200 [==============================] - 0s 137us/step - loss: 0.3653 - acc: 0.8800 - val_loss: 0.4258 - val_acc: 0.8250\n",
      "Epoch 170/500\n",
      "200/200 [==============================] - 0s 173us/step - loss: 0.3641 - acc: 0.8750 - val_loss: 0.4256 - val_acc: 0.8250\n",
      "Epoch 171/500\n",
      "200/200 [==============================] - 0s 170us/step - loss: 0.3633 - acc: 0.8800 - val_loss: 0.4255 - val_acc: 0.8200\n",
      "Epoch 172/500\n",
      "200/200 [==============================] - 0s 164us/step - loss: 0.3629 - acc: 0.8800 - val_loss: 0.4257 - val_acc: 0.8175\n",
      "Epoch 173/500\n",
      "200/200 [==============================] - 0s 181us/step - loss: 0.3633 - acc: 0.8800 - val_loss: 0.4262 - val_acc: 0.8187\n",
      "Epoch 174/500\n",
      "200/200 [==============================] - 0s 146us/step - loss: 0.3624 - acc: 0.8800 - val_loss: 0.4265 - val_acc: 0.8200\n",
      "Epoch 175/500\n",
      "200/200 [==============================] - 0s 145us/step - loss: 0.3616 - acc: 0.8800 - val_loss: 0.4280 - val_acc: 0.8175\n",
      "Epoch 176/500\n",
      "200/200 [==============================] - 0s 192us/step - loss: 0.3622 - acc: 0.8800 - val_loss: 0.4299 - val_acc: 0.8200\n",
      "Epoch 177/500\n",
      "200/200 [==============================] - 0s 176us/step - loss: 0.3625 - acc: 0.8750 - val_loss: 0.4294 - val_acc: 0.8187\n",
      "Epoch 178/500\n",
      "200/200 [==============================] - 0s 181us/step - loss: 0.3615 - acc: 0.8750 - val_loss: 0.4276 - val_acc: 0.8200\n",
      "Epoch 179/500\n",
      "200/200 [==============================] - 0s 217us/step - loss: 0.3597 - acc: 0.8750 - val_loss: 0.4276 - val_acc: 0.8187\n",
      "Epoch 180/500\n",
      "200/200 [==============================] - 0s 164us/step - loss: 0.3598 - acc: 0.8700 - val_loss: 0.4266 - val_acc: 0.8187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/500\n",
      "200/200 [==============================] - 0s 176us/step - loss: 0.3583 - acc: 0.8700 - val_loss: 0.4242 - val_acc: 0.8187\n",
      "Epoch 182/500\n",
      "200/200 [==============================] - 0s 191us/step - loss: 0.3598 - acc: 0.8750 - val_loss: 0.4201 - val_acc: 0.8237\n",
      "Epoch 183/500\n",
      "200/200 [==============================] - 0s 166us/step - loss: 0.3577 - acc: 0.8750 - val_loss: 0.4186 - val_acc: 0.8225\n",
      "Epoch 184/500\n",
      "200/200 [==============================] - 0s 195us/step - loss: 0.3571 - acc: 0.8750 - val_loss: 0.4180 - val_acc: 0.8237\n",
      "Epoch 185/500\n",
      "200/200 [==============================] - 0s 175us/step - loss: 0.3570 - acc: 0.8750 - val_loss: 0.4172 - val_acc: 0.8250\n",
      "Epoch 186/500\n",
      "200/200 [==============================] - 0s 194us/step - loss: 0.3563 - acc: 0.8750 - val_loss: 0.4172 - val_acc: 0.8237\n",
      "Epoch 187/500\n",
      "200/200 [==============================] - 0s 171us/step - loss: 0.3559 - acc: 0.8750 - val_loss: 0.4181 - val_acc: 0.8225\n",
      "Epoch 188/500\n",
      "200/200 [==============================] - 0s 165us/step - loss: 0.3568 - acc: 0.8750 - val_loss: 0.4181 - val_acc: 0.8263\n",
      "Epoch 189/500\n",
      "200/200 [==============================] - 0s 192us/step - loss: 0.3553 - acc: 0.8800 - val_loss: 0.4191 - val_acc: 0.8250\n",
      "Epoch 190/500\n",
      "200/200 [==============================] - 0s 179us/step - loss: 0.3537 - acc: 0.8750 - val_loss: 0.4194 - val_acc: 0.8237\n",
      "Epoch 191/500\n",
      "200/200 [==============================] - 0s 182us/step - loss: 0.3534 - acc: 0.8750 - val_loss: 0.4194 - val_acc: 0.8225\n",
      "Epoch 192/500\n",
      "200/200 [==============================] - 0s 134us/step - loss: 0.3530 - acc: 0.8800 - val_loss: 0.4203 - val_acc: 0.8237\n",
      "Epoch 193/500\n",
      "200/200 [==============================] - 0s 189us/step - loss: 0.3527 - acc: 0.8700 - val_loss: 0.4209 - val_acc: 0.8213\n",
      "Epoch 194/500\n",
      "200/200 [==============================] - 0s 192us/step - loss: 0.3519 - acc: 0.8700 - val_loss: 0.4209 - val_acc: 0.8175\n",
      "Epoch 195/500\n",
      "200/200 [==============================] - 0s 175us/step - loss: 0.3518 - acc: 0.8800 - val_loss: 0.4215 - val_acc: 0.8163\n",
      "Epoch 196/500\n",
      "200/200 [==============================] - 0s 171us/step - loss: 0.3514 - acc: 0.8800 - val_loss: 0.4211 - val_acc: 0.8163\n",
      "Epoch 197/500\n",
      "200/200 [==============================] - 0s 199us/step - loss: 0.3509 - acc: 0.8800 - val_loss: 0.4213 - val_acc: 0.8187\n",
      "Epoch 198/500\n",
      "200/200 [==============================] - 0s 174us/step - loss: 0.3509 - acc: 0.8800 - val_loss: 0.4208 - val_acc: 0.8213\n",
      "Epoch 199/500\n",
      "200/200 [==============================] - 0s 169us/step - loss: 0.3502 - acc: 0.8850 - val_loss: 0.4192 - val_acc: 0.8237\n",
      "Epoch 200/500\n",
      "200/200 [==============================] - 0s 173us/step - loss: 0.3502 - acc: 0.8850 - val_loss: 0.4177 - val_acc: 0.8263\n",
      "Epoch 201/500\n",
      "200/200 [==============================] - 0s 181us/step - loss: 0.3495 - acc: 0.8850 - val_loss: 0.4181 - val_acc: 0.8225\n",
      "Epoch 202/500\n",
      "200/200 [==============================] - 0s 181us/step - loss: 0.3491 - acc: 0.8800 - val_loss: 0.4171 - val_acc: 0.8225\n",
      "Epoch 203/500\n",
      "200/200 [==============================] - 0s 202us/step - loss: 0.3486 - acc: 0.8800 - val_loss: 0.4165 - val_acc: 0.8250\n",
      "Epoch 204/500\n",
      "200/200 [==============================] - 0s 149us/step - loss: 0.3478 - acc: 0.8800 - val_loss: 0.4167 - val_acc: 0.8237\n",
      "Epoch 205/500\n",
      "200/200 [==============================] - 0s 188us/step - loss: 0.3472 - acc: 0.8750 - val_loss: 0.4164 - val_acc: 0.8213\n",
      "Epoch 206/500\n",
      "200/200 [==============================] - 0s 168us/step - loss: 0.3475 - acc: 0.8750 - val_loss: 0.4175 - val_acc: 0.8200\n",
      "Epoch 207/500\n",
      "200/200 [==============================] - 0s 181us/step - loss: 0.3470 - acc: 0.8800 - val_loss: 0.4171 - val_acc: 0.8263\n",
      "Epoch 208/500\n",
      "200/200 [==============================] - 0s 189us/step - loss: 0.3464 - acc: 0.8850 - val_loss: 0.4166 - val_acc: 0.8263\n",
      "Epoch 209/500\n",
      "200/200 [==============================] - 0s 155us/step - loss: 0.3472 - acc: 0.8850 - val_loss: 0.4158 - val_acc: 0.8275\n",
      "Epoch 210/500\n",
      "200/200 [==============================] - 0s 159us/step - loss: 0.3458 - acc: 0.8850 - val_loss: 0.4158 - val_acc: 0.8263\n",
      "Epoch 211/500\n",
      "200/200 [==============================] - 0s 168us/step - loss: 0.3455 - acc: 0.8800 - val_loss: 0.4159 - val_acc: 0.8237\n",
      "Epoch 212/500\n",
      "200/200 [==============================] - 0s 152us/step - loss: 0.3449 - acc: 0.8800 - val_loss: 0.4149 - val_acc: 0.8225\n",
      "Epoch 213/500\n",
      "200/200 [==============================] - 0s 187us/step - loss: 0.3451 - acc: 0.8800 - val_loss: 0.4138 - val_acc: 0.8225\n",
      "Epoch 214/500\n",
      "200/200 [==============================] - 0s 164us/step - loss: 0.3444 - acc: 0.8850 - val_loss: 0.4136 - val_acc: 0.8225\n",
      "Epoch 215/500\n",
      "200/200 [==============================] - 0s 114us/step - loss: 0.3450 - acc: 0.8750 - val_loss: 0.4134 - val_acc: 0.8225\n",
      "Epoch 216/500\n",
      "200/200 [==============================] - 0s 190us/step - loss: 0.3443 - acc: 0.8850 - val_loss: 0.4144 - val_acc: 0.8263\n",
      "Epoch 217/500\n",
      "200/200 [==============================] - 0s 165us/step - loss: 0.3440 - acc: 0.8850 - val_loss: 0.4147 - val_acc: 0.8250\n",
      "Epoch 218/500\n",
      "200/200 [==============================] - 0s 168us/step - loss: 0.3434 - acc: 0.8800 - val_loss: 0.4142 - val_acc: 0.8263\n",
      "Epoch 219/500\n",
      "200/200 [==============================] - 0s 161us/step - loss: 0.3424 - acc: 0.8800 - val_loss: 0.4137 - val_acc: 0.8225\n",
      "Epoch 220/500\n",
      "200/200 [==============================] - 0s 145us/step - loss: 0.3426 - acc: 0.8800 - val_loss: 0.4132 - val_acc: 0.8225\n",
      "Epoch 221/500\n",
      "200/200 [==============================] - 0s 136us/step - loss: 0.3418 - acc: 0.8800 - val_loss: 0.4121 - val_acc: 0.8237\n",
      "Epoch 222/500\n",
      "200/200 [==============================] - 0s 170us/step - loss: 0.3412 - acc: 0.8800 - val_loss: 0.4111 - val_acc: 0.8263\n",
      "Epoch 223/500\n",
      "200/200 [==============================] - 0s 156us/step - loss: 0.3416 - acc: 0.8800 - val_loss: 0.4106 - val_acc: 0.8263\n",
      "Epoch 224/500\n",
      "200/200 [==============================] - 0s 185us/step - loss: 0.3412 - acc: 0.8800 - val_loss: 0.4098 - val_acc: 0.8263\n",
      "Epoch 225/500\n",
      "200/200 [==============================] - 0s 155us/step - loss: 0.3407 - acc: 0.8750 - val_loss: 0.4095 - val_acc: 0.8263\n",
      "Epoch 226/500\n",
      "200/200 [==============================] - 0s 188us/step - loss: 0.3394 - acc: 0.8850 - val_loss: 0.4107 - val_acc: 0.8313\n",
      "Epoch 227/500\n",
      "200/200 [==============================] - 0s 208us/step - loss: 0.3417 - acc: 0.8900 - val_loss: 0.4124 - val_acc: 0.8313\n",
      "Epoch 228/500\n",
      "200/200 [==============================] - 0s 174us/step - loss: 0.3407 - acc: 0.8900 - val_loss: 0.4118 - val_acc: 0.8313\n",
      "Epoch 229/500\n",
      "200/200 [==============================] - 0s 185us/step - loss: 0.3405 - acc: 0.8800 - val_loss: 0.4113 - val_acc: 0.8225\n",
      "Epoch 230/500\n",
      "200/200 [==============================] - 0s 175us/step - loss: 0.3389 - acc: 0.8800 - val_loss: 0.4102 - val_acc: 0.8263\n",
      "Epoch 231/500\n",
      "200/200 [==============================] - 0s 157us/step - loss: 0.3388 - acc: 0.8800 - val_loss: 0.4094 - val_acc: 0.8225\n",
      "Epoch 232/500\n",
      "200/200 [==============================] - 0s 165us/step - loss: 0.3385 - acc: 0.8750 - val_loss: 0.4097 - val_acc: 0.8250\n",
      "Epoch 233/500\n",
      "200/200 [==============================] - 0s 161us/step - loss: 0.3381 - acc: 0.8800 - val_loss: 0.4094 - val_acc: 0.8237\n",
      "Epoch 234/500\n",
      "200/200 [==============================] - 0s 174us/step - loss: 0.3383 - acc: 0.8750 - val_loss: 0.4094 - val_acc: 0.8250\n",
      "Epoch 235/500\n",
      "200/200 [==============================] - 0s 172us/step - loss: 0.3380 - acc: 0.8750 - val_loss: 0.4101 - val_acc: 0.8213\n",
      "Epoch 236/500\n",
      "200/200 [==============================] - 0s 158us/step - loss: 0.3376 - acc: 0.8750 - val_loss: 0.4111 - val_acc: 0.8187\n",
      "Epoch 237/500\n",
      "200/200 [==============================] - 0s 169us/step - loss: 0.3370 - acc: 0.8750 - val_loss: 0.4113 - val_acc: 0.8200\n",
      "Epoch 238/500\n",
      "200/200 [==============================] - 0s 196us/step - loss: 0.3365 - acc: 0.8800 - val_loss: 0.4122 - val_acc: 0.8250\n",
      "Epoch 239/500\n",
      "200/200 [==============================] - 0s 177us/step - loss: 0.3364 - acc: 0.8800 - val_loss: 0.4117 - val_acc: 0.8263\n",
      "Epoch 240/500\n",
      "200/200 [==============================] - 0s 176us/step - loss: 0.3359 - acc: 0.8800 - val_loss: 0.4121 - val_acc: 0.8263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/500\n",
      "200/200 [==============================] - 0s 178us/step - loss: 0.3356 - acc: 0.8800 - val_loss: 0.4128 - val_acc: 0.8213\n",
      "Epoch 242/500\n",
      "200/200 [==============================] - 0s 164us/step - loss: 0.3360 - acc: 0.8800 - val_loss: 0.4131 - val_acc: 0.8237\n",
      "Epoch 243/500\n",
      "200/200 [==============================] - 0s 206us/step - loss: 0.3360 - acc: 0.8900 - val_loss: 0.4131 - val_acc: 0.8250\n",
      "Epoch 244/500\n",
      "200/200 [==============================] - 0s 189us/step - loss: 0.3352 - acc: 0.8900 - val_loss: 0.4129 - val_acc: 0.8275\n",
      "Epoch 245/500\n",
      "200/200 [==============================] - 0s 181us/step - loss: 0.3351 - acc: 0.8900 - val_loss: 0.4122 - val_acc: 0.8250\n",
      "Epoch 246/500\n",
      "200/200 [==============================] - 0s 189us/step - loss: 0.3344 - acc: 0.8900 - val_loss: 0.4121 - val_acc: 0.8237\n",
      "Epoch 247/500\n",
      "200/200 [==============================] - 0s 176us/step - loss: 0.3349 - acc: 0.8800 - val_loss: 0.4118 - val_acc: 0.8187\n",
      "Epoch 248/500\n",
      "200/200 [==============================] - 0s 176us/step - loss: 0.3344 - acc: 0.8800 - val_loss: 0.4106 - val_acc: 0.8250\n",
      "Epoch 249/500\n",
      "200/200 [==============================] - 0s 172us/step - loss: 0.3338 - acc: 0.8800 - val_loss: 0.4109 - val_acc: 0.8213\n",
      "Epoch 250/500\n",
      "200/200 [==============================] - 0s 158us/step - loss: 0.3338 - acc: 0.8800 - val_loss: 0.4106 - val_acc: 0.8213\n",
      "Epoch 251/500\n",
      "200/200 [==============================] - 0s 224us/step - loss: 0.3333 - acc: 0.8800 - val_loss: 0.4101 - val_acc: 0.8213\n",
      "Epoch 252/500\n",
      "200/200 [==============================] - 0s 272us/step - loss: 0.3324 - acc: 0.8800 - val_loss: 0.4100 - val_acc: 0.8213\n",
      "Epoch 253/500\n",
      "200/200 [==============================] - 0s 304us/step - loss: 0.3326 - acc: 0.8800 - val_loss: 0.4094 - val_acc: 0.8200\n",
      "Epoch 254/500\n",
      "200/200 [==============================] - 0s 244us/step - loss: 0.3327 - acc: 0.8800 - val_loss: 0.4097 - val_acc: 0.8200\n",
      "Epoch 255/500\n",
      "200/200 [==============================] - 0s 275us/step - loss: 0.3325 - acc: 0.8800 - val_loss: 0.4088 - val_acc: 0.8237\n",
      "Epoch 256/500\n",
      "200/200 [==============================] - 0s 145us/step - loss: 0.3326 - acc: 0.8800 - val_loss: 0.4089 - val_acc: 0.8213\n",
      "Epoch 257/500\n",
      "200/200 [==============================] - 0s 227us/step - loss: 0.3324 - acc: 0.8800 - val_loss: 0.4102 - val_acc: 0.8225\n",
      "Epoch 258/500\n",
      "200/200 [==============================] - 0s 188us/step - loss: 0.3317 - acc: 0.8850 - val_loss: 0.4101 - val_acc: 0.8225\n",
      "Epoch 259/500\n",
      "200/200 [==============================] - 0s 181us/step - loss: 0.3324 - acc: 0.8800 - val_loss: 0.4095 - val_acc: 0.8225\n",
      "Epoch 260/500\n",
      "200/200 [==============================] - 0s 152us/step - loss: 0.3314 - acc: 0.8800 - val_loss: 0.4095 - val_acc: 0.8225\n",
      "Epoch 261/500\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.1746 - acc: 0.937 - 0s 161us/step - loss: 0.3305 - acc: 0.8800 - val_loss: 0.4085 - val_acc: 0.8237\n",
      "Epoch 262/500\n",
      "200/200 [==============================] - 0s 175us/step - loss: 0.3306 - acc: 0.8800 - val_loss: 0.4078 - val_acc: 0.8287\n",
      "Epoch 263/500\n",
      "200/200 [==============================] - 0s 177us/step - loss: 0.3298 - acc: 0.8900 - val_loss: 0.4057 - val_acc: 0.8300\n",
      "Epoch 264/500\n",
      "200/200 [==============================] - 0s 218us/step - loss: 0.3304 - acc: 0.8800 - val_loss: 0.4039 - val_acc: 0.8337\n",
      "Epoch 265/500\n",
      "200/200 [==============================] - 0s 213us/step - loss: 0.3299 - acc: 0.8800 - val_loss: 0.4050 - val_acc: 0.8287\n",
      "Epoch 266/500\n",
      "200/200 [==============================] - 0s 193us/step - loss: 0.3299 - acc: 0.8800 - val_loss: 0.4068 - val_acc: 0.8250\n",
      "Epoch 267/500\n",
      "200/200 [==============================] - 0s 215us/step - loss: 0.3299 - acc: 0.8800 - val_loss: 0.4070 - val_acc: 0.8225\n",
      "Epoch 268/500\n",
      "200/200 [==============================] - 0s 170us/step - loss: 0.3287 - acc: 0.8800 - val_loss: 0.4089 - val_acc: 0.8287\n",
      "Epoch 269/500\n",
      "200/200 [==============================] - 0s 191us/step - loss: 0.3292 - acc: 0.8900 - val_loss: 0.4099 - val_acc: 0.8250\n",
      "Epoch 270/500\n",
      "200/200 [==============================] - 0s 149us/step - loss: 0.3289 - acc: 0.8900 - val_loss: 0.4099 - val_acc: 0.8250\n",
      "Epoch 271/500\n",
      "200/200 [==============================] - 0s 133us/step - loss: 0.3285 - acc: 0.8900 - val_loss: 0.4081 - val_acc: 0.8250\n",
      "Epoch 272/500\n",
      "200/200 [==============================] - 0s 157us/step - loss: 0.3287 - acc: 0.8800 - val_loss: 0.4074 - val_acc: 0.8263\n",
      "Epoch 273/500\n",
      "200/200 [==============================] - 0s 153us/step - loss: 0.3283 - acc: 0.8850 - val_loss: 0.4080 - val_acc: 0.8287\n",
      "Epoch 274/500\n",
      "200/200 [==============================] - 0s 188us/step - loss: 0.3279 - acc: 0.8850 - val_loss: 0.4087 - val_acc: 0.8287\n",
      "Epoch 275/500\n",
      "200/200 [==============================] - 0s 183us/step - loss: 0.3278 - acc: 0.8900 - val_loss: 0.4088 - val_acc: 0.8275\n",
      "Epoch 276/500\n",
      "200/200 [==============================] - 0s 150us/step - loss: 0.3281 - acc: 0.8850 - val_loss: 0.4084 - val_acc: 0.8263\n",
      "Epoch 277/500\n",
      "200/200 [==============================] - 0s 144us/step - loss: 0.3276 - acc: 0.8850 - val_loss: 0.4080 - val_acc: 0.8275\n",
      "Epoch 278/500\n",
      "200/200 [==============================] - 0s 174us/step - loss: 0.3273 - acc: 0.8800 - val_loss: 0.4105 - val_acc: 0.8213\n",
      "Epoch 279/500\n",
      "200/200 [==============================] - 0s 158us/step - loss: 0.3272 - acc: 0.8800 - val_loss: 0.4108 - val_acc: 0.8250\n",
      "Epoch 280/500\n",
      "200/200 [==============================] - 0s 195us/step - loss: 0.3274 - acc: 0.8800 - val_loss: 0.4098 - val_acc: 0.8250\n",
      "Epoch 281/500\n",
      "200/200 [==============================] - 0s 177us/step - loss: 0.3267 - acc: 0.8800 - val_loss: 0.4087 - val_acc: 0.8263\n",
      "Epoch 282/500\n",
      "200/200 [==============================] - 0s 142us/step - loss: 0.3263 - acc: 0.8800 - val_loss: 0.4095 - val_acc: 0.8250\n",
      "Epoch 283/500\n",
      "200/200 [==============================] - 0s 166us/step - loss: 0.3266 - acc: 0.8850 - val_loss: 0.4113 - val_acc: 0.8237\n",
      "Epoch 284/500\n",
      "200/200 [==============================] - 0s 150us/step - loss: 0.3263 - acc: 0.8800 - val_loss: 0.4102 - val_acc: 0.8213\n",
      "Epoch 285/500\n",
      "200/200 [==============================] - 0s 159us/step - loss: 0.3262 - acc: 0.8800 - val_loss: 0.4094 - val_acc: 0.8213\n",
      "Epoch 286/500\n",
      "200/200 [==============================] - 0s 164us/step - loss: 0.3256 - acc: 0.8800 - val_loss: 0.4098 - val_acc: 0.8250\n",
      "Epoch 287/500\n",
      "200/200 [==============================] - 0s 179us/step - loss: 0.3260 - acc: 0.8900 - val_loss: 0.4111 - val_acc: 0.8225\n",
      "Epoch 288/500\n",
      "200/200 [==============================] - 0s 204us/step - loss: 0.3262 - acc: 0.8800 - val_loss: 0.4108 - val_acc: 0.8225\n",
      "Epoch 289/500\n",
      "200/200 [==============================] - 0s 150us/step - loss: 0.3254 - acc: 0.8800 - val_loss: 0.4098 - val_acc: 0.8225\n",
      "Epoch 290/500\n",
      "200/200 [==============================] - 0s 165us/step - loss: 0.3248 - acc: 0.8800 - val_loss: 0.4082 - val_acc: 0.8200\n",
      "Epoch 291/500\n",
      "200/200 [==============================] - 0s 191us/step - loss: 0.3254 - acc: 0.8850 - val_loss: 0.4093 - val_acc: 0.8213\n",
      "Epoch 292/500\n",
      "200/200 [==============================] - 0s 184us/step - loss: 0.3252 - acc: 0.8850 - val_loss: 0.4096 - val_acc: 0.8225\n",
      "Epoch 293/500\n",
      "200/200 [==============================] - 0s 164us/step - loss: 0.3247 - acc: 0.8800 - val_loss: 0.4124 - val_acc: 0.8250\n",
      "Epoch 294/500\n",
      "200/200 [==============================] - 0s 175us/step - loss: 0.3253 - acc: 0.8900 - val_loss: 0.4106 - val_acc: 0.8275\n",
      "Epoch 295/500\n",
      "200/200 [==============================] - 0s 159us/step - loss: 0.3244 - acc: 0.8900 - val_loss: 0.4100 - val_acc: 0.8287\n",
      "Epoch 296/500\n",
      "200/200 [==============================] - 0s 205us/step - loss: 0.3242 - acc: 0.8900 - val_loss: 0.4097 - val_acc: 0.8275\n",
      "Epoch 297/500\n",
      "200/200 [==============================] - 0s 156us/step - loss: 0.3243 - acc: 0.8900 - val_loss: 0.4084 - val_acc: 0.8275\n",
      "Epoch 298/500\n",
      "200/200 [==============================] - 0s 155us/step - loss: 0.3233 - acc: 0.8900 - val_loss: 0.4087 - val_acc: 0.8263\n",
      "Epoch 299/500\n",
      "200/200 [==============================] - 0s 151us/step - loss: 0.3239 - acc: 0.8800 - val_loss: 0.4093 - val_acc: 0.8237\n",
      "Epoch 300/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 151us/step - loss: 0.3235 - acc: 0.8800 - val_loss: 0.4088 - val_acc: 0.8237\n",
      "Epoch 301/500\n",
      "200/200 [==============================] - 0s 132us/step - loss: 0.3232 - acc: 0.8800 - val_loss: 0.4075 - val_acc: 0.8250\n",
      "Epoch 302/500\n",
      "200/200 [==============================] - 0s 157us/step - loss: 0.3227 - acc: 0.8850 - val_loss: 0.4064 - val_acc: 0.8275\n",
      "Epoch 303/500\n",
      "200/200 [==============================] - 0s 184us/step - loss: 0.3226 - acc: 0.8850 - val_loss: 0.4071 - val_acc: 0.8263\n",
      "Epoch 304/500\n",
      "200/200 [==============================] - 0s 173us/step - loss: 0.3222 - acc: 0.8900 - val_loss: 0.4088 - val_acc: 0.8250\n",
      "Epoch 305/500\n",
      "200/200 [==============================] - 0s 184us/step - loss: 0.3227 - acc: 0.8900 - val_loss: 0.4094 - val_acc: 0.8237\n",
      "Epoch 306/500\n",
      "200/200 [==============================] - 0s 155us/step - loss: 0.3222 - acc: 0.8900 - val_loss: 0.4095 - val_acc: 0.8250\n",
      "Epoch 307/500\n",
      "200/200 [==============================] - 0s 147us/step - loss: 0.3222 - acc: 0.8850 - val_loss: 0.4100 - val_acc: 0.8225\n",
      "Epoch 308/500\n",
      "200/200 [==============================] - 0s 178us/step - loss: 0.3221 - acc: 0.8800 - val_loss: 0.4104 - val_acc: 0.8213\n",
      "Epoch 309/500\n",
      "200/200 [==============================] - 0s 194us/step - loss: 0.3221 - acc: 0.8800 - val_loss: 0.4100 - val_acc: 0.8250\n",
      "Epoch 310/500\n",
      "200/200 [==============================] - 0s 164us/step - loss: 0.3217 - acc: 0.8850 - val_loss: 0.4083 - val_acc: 0.8250\n",
      "Epoch 311/500\n",
      "200/200 [==============================] - 0s 233us/step - loss: 0.3218 - acc: 0.8800 - val_loss: 0.4088 - val_acc: 0.8250\n",
      "Epoch 312/500\n",
      "200/200 [==============================] - 0s 152us/step - loss: 0.3213 - acc: 0.8800 - val_loss: 0.4078 - val_acc: 0.8225\n",
      "Epoch 313/500\n",
      "200/200 [==============================] - 0s 195us/step - loss: 0.3212 - acc: 0.8800 - val_loss: 0.4066 - val_acc: 0.8275\n",
      "Epoch 314/500\n",
      "200/200 [==============================] - 0s 159us/step - loss: 0.3220 - acc: 0.8750 - val_loss: 0.4062 - val_acc: 0.8250\n",
      "Epoch 315/500\n",
      "200/200 [==============================] - 0s 196us/step - loss: 0.3206 - acc: 0.8850 - val_loss: 0.4075 - val_acc: 0.8275\n",
      "Epoch 316/500\n",
      "200/200 [==============================] - 0s 178us/step - loss: 0.3213 - acc: 0.8850 - val_loss: 0.4087 - val_acc: 0.8287\n",
      "Epoch 317/500\n",
      "200/200 [==============================] - 0s 160us/step - loss: 0.3210 - acc: 0.8900 - val_loss: 0.4090 - val_acc: 0.8275\n",
      "Epoch 318/500\n",
      "200/200 [==============================] - 0s 186us/step - loss: 0.3205 - acc: 0.8900 - val_loss: 0.4084 - val_acc: 0.8263\n",
      "Epoch 319/500\n",
      "200/200 [==============================] - 0s 208us/step - loss: 0.3205 - acc: 0.8800 - val_loss: 0.4067 - val_acc: 0.8287\n",
      "Epoch 320/500\n",
      "200/200 [==============================] - 0s 183us/step - loss: 0.3199 - acc: 0.8800 - val_loss: 0.4079 - val_acc: 0.8250\n",
      "Epoch 321/500\n",
      "200/200 [==============================] - 0s 204us/step - loss: 0.3210 - acc: 0.8900 - val_loss: 0.4104 - val_acc: 0.8263\n",
      "Epoch 322/500\n",
      "200/200 [==============================] - 0s 190us/step - loss: 0.3201 - acc: 0.8900 - val_loss: 0.4126 - val_acc: 0.8250\n",
      "Epoch 323/500\n",
      "200/200 [==============================] - 0s 182us/step - loss: 0.3202 - acc: 0.8850 - val_loss: 0.4133 - val_acc: 0.8237\n",
      "Epoch 324/500\n",
      "200/200 [==============================] - 0s 173us/step - loss: 0.3206 - acc: 0.8800 - val_loss: 0.4150 - val_acc: 0.8213\n",
      "Epoch 325/500\n",
      "200/200 [==============================] - 0s 177us/step - loss: 0.3198 - acc: 0.8800 - val_loss: 0.4149 - val_acc: 0.8225\n",
      "Epoch 326/500\n",
      "200/200 [==============================] - 0s 220us/step - loss: 0.3194 - acc: 0.8800 - val_loss: 0.4151 - val_acc: 0.8250\n",
      "Epoch 327/500\n",
      "200/200 [==============================] - 0s 208us/step - loss: 0.3196 - acc: 0.8900 - val_loss: 0.4147 - val_acc: 0.8250\n",
      "Epoch 328/500\n",
      "200/200 [==============================] - 0s 159us/step - loss: 0.3199 - acc: 0.8850 - val_loss: 0.4125 - val_acc: 0.8263\n",
      "Epoch 329/500\n",
      "200/200 [==============================] - 0s 180us/step - loss: 0.3187 - acc: 0.8900 - val_loss: 0.4134 - val_acc: 0.8263\n",
      "Epoch 330/500\n",
      "200/200 [==============================] - 0s 171us/step - loss: 0.3198 - acc: 0.8850 - val_loss: 0.4140 - val_acc: 0.8250\n",
      "Epoch 331/500\n",
      "200/200 [==============================] - 0s 151us/step - loss: 0.3197 - acc: 0.8850 - val_loss: 0.4142 - val_acc: 0.8237\n",
      "Epoch 332/500\n",
      "200/200 [==============================] - 0s 166us/step - loss: 0.3200 - acc: 0.8800 - val_loss: 0.4130 - val_acc: 0.8213\n",
      "Epoch 333/500\n",
      "200/200 [==============================] - 0s 137us/step - loss: 0.3185 - acc: 0.8800 - val_loss: 0.4115 - val_acc: 0.8237\n",
      "Epoch 334/500\n",
      "200/200 [==============================] - 0s 139us/step - loss: 0.3189 - acc: 0.8800 - val_loss: 0.4097 - val_acc: 0.8250\n",
      "Epoch 335/500\n",
      "200/200 [==============================] - 0s 184us/step - loss: 0.3183 - acc: 0.8750 - val_loss: 0.4094 - val_acc: 0.8200\n",
      "Epoch 336/500\n",
      "200/200 [==============================] - 0s 148us/step - loss: 0.3185 - acc: 0.8800 - val_loss: 0.4089 - val_acc: 0.8213\n",
      "Epoch 337/500\n",
      "200/200 [==============================] - 0s 158us/step - loss: 0.3184 - acc: 0.8800 - val_loss: 0.4086 - val_acc: 0.8187\n",
      "Epoch 338/500\n",
      "200/200 [==============================] - 0s 160us/step - loss: 0.3177 - acc: 0.8800 - val_loss: 0.4087 - val_acc: 0.8225\n",
      "Epoch 339/500\n",
      "200/200 [==============================] - 0s 178us/step - loss: 0.3165 - acc: 0.8800 - val_loss: 0.4099 - val_acc: 0.8275\n",
      "Epoch 340/500\n",
      "200/200 [==============================] - 0s 197us/step - loss: 0.3199 - acc: 0.8800 - val_loss: 0.4097 - val_acc: 0.8250\n",
      "Epoch 341/500\n",
      "200/200 [==============================] - 0s 180us/step - loss: 0.3188 - acc: 0.8800 - val_loss: 0.4093 - val_acc: 0.8300\n",
      "Epoch 342/500\n",
      "200/200 [==============================] - 0s 181us/step - loss: 0.3183 - acc: 0.8800 - val_loss: 0.4089 - val_acc: 0.8263\n",
      "Epoch 343/500\n",
      "200/200 [==============================] - 0s 168us/step - loss: 0.3173 - acc: 0.8850 - val_loss: 0.4093 - val_acc: 0.8225\n",
      "Epoch 344/500\n",
      "200/200 [==============================] - 0s 127us/step - loss: 0.3169 - acc: 0.8800 - val_loss: 0.4108 - val_acc: 0.8225\n",
      "Epoch 345/500\n",
      "200/200 [==============================] - 0s 205us/step - loss: 0.3173 - acc: 0.8800 - val_loss: 0.4113 - val_acc: 0.8237\n",
      "Epoch 346/500\n",
      "200/200 [==============================] - 0s 160us/step - loss: 0.3173 - acc: 0.8800 - val_loss: 0.4118 - val_acc: 0.8250\n",
      "Epoch 347/500\n",
      "200/200 [==============================] - 0s 166us/step - loss: 0.3166 - acc: 0.8750 - val_loss: 0.4106 - val_acc: 0.8237\n",
      "Epoch 348/500\n",
      "200/200 [==============================] - 0s 151us/step - loss: 0.3162 - acc: 0.8850 - val_loss: 0.4105 - val_acc: 0.8225\n",
      "Epoch 349/500\n",
      "200/200 [==============================] - 0s 147us/step - loss: 0.3168 - acc: 0.8900 - val_loss: 0.4093 - val_acc: 0.8275\n",
      "Epoch 350/500\n",
      "200/200 [==============================] - 0s 167us/step - loss: 0.3173 - acc: 0.8800 - val_loss: 0.4065 - val_acc: 0.8275\n",
      "Epoch 351/500\n",
      "200/200 [==============================] - 0s 179us/step - loss: 0.3168 - acc: 0.8800 - val_loss: 0.4060 - val_acc: 0.8287\n",
      "Epoch 352/500\n",
      "200/200 [==============================] - 0s 141us/step - loss: 0.3167 - acc: 0.8800 - val_loss: 0.4058 - val_acc: 0.8287\n",
      "Epoch 353/500\n",
      "200/200 [==============================] - 0s 134us/step - loss: 0.3161 - acc: 0.8800 - val_loss: 0.4067 - val_acc: 0.8275\n",
      "Epoch 354/500\n",
      "200/200 [==============================] - 0s 164us/step - loss: 0.3161 - acc: 0.8800 - val_loss: 0.4076 - val_acc: 0.8287\n",
      "Epoch 355/500\n",
      "200/200 [==============================] - 0s 132us/step - loss: 0.3158 - acc: 0.8900 - val_loss: 0.4083 - val_acc: 0.8263\n",
      "Epoch 356/500\n",
      "200/200 [==============================] - 0s 184us/step - loss: 0.3152 - acc: 0.8850 - val_loss: 0.4080 - val_acc: 0.8275\n",
      "Epoch 357/500\n",
      "200/200 [==============================] - 0s 160us/step - loss: 0.3151 - acc: 0.8850 - val_loss: 0.4075 - val_acc: 0.8300\n",
      "Epoch 358/500\n",
      "200/200 [==============================] - 0s 161us/step - loss: 0.3156 - acc: 0.8850 - val_loss: 0.4080 - val_acc: 0.8300\n",
      "Epoch 359/500\n",
      "200/200 [==============================] - 0s 131us/step - loss: 0.3152 - acc: 0.8850 - val_loss: 0.4089 - val_acc: 0.8250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360/500\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.3154 - acc: 0.906 - 0s 139us/step - loss: 0.3150 - acc: 0.8850 - val_loss: 0.4094 - val_acc: 0.8237\n",
      "Epoch 361/500\n",
      "200/200 [==============================] - 0s 188us/step - loss: 0.3148 - acc: 0.8750 - val_loss: 0.4102 - val_acc: 0.8237\n",
      "Epoch 362/500\n",
      "200/200 [==============================] - 0s 173us/step - loss: 0.3146 - acc: 0.8850 - val_loss: 0.4100 - val_acc: 0.8275\n",
      "Epoch 363/500\n",
      "200/200 [==============================] - 0s 172us/step - loss: 0.3146 - acc: 0.8850 - val_loss: 0.4109 - val_acc: 0.8275\n",
      "Epoch 364/500\n",
      "200/200 [==============================] - 0s 168us/step - loss: 0.3144 - acc: 0.8850 - val_loss: 0.4100 - val_acc: 0.8263\n",
      "Epoch 365/500\n",
      "200/200 [==============================] - 0s 195us/step - loss: 0.3144 - acc: 0.8850 - val_loss: 0.4102 - val_acc: 0.8263\n",
      "Epoch 366/500\n",
      "200/200 [==============================] - 0s 166us/step - loss: 0.3146 - acc: 0.8800 - val_loss: 0.4093 - val_acc: 0.8263\n",
      "Epoch 367/500\n",
      "200/200 [==============================] - 0s 169us/step - loss: 0.3149 - acc: 0.8800 - val_loss: 0.4100 - val_acc: 0.8237\n",
      "Epoch 368/500\n",
      "200/200 [==============================] - 0s 163us/step - loss: 0.3145 - acc: 0.8800 - val_loss: 0.4094 - val_acc: 0.8263\n",
      "Epoch 369/500\n",
      "200/200 [==============================] - 0s 166us/step - loss: 0.3140 - acc: 0.8900 - val_loss: 0.4104 - val_acc: 0.8275\n",
      "Epoch 370/500\n",
      "200/200 [==============================] - 0s 150us/step - loss: 0.3137 - acc: 0.8850 - val_loss: 0.4117 - val_acc: 0.8250\n",
      "Epoch 371/500\n",
      "200/200 [==============================] - 0s 179us/step - loss: 0.3135 - acc: 0.8850 - val_loss: 0.4123 - val_acc: 0.8237\n",
      "Epoch 372/500\n",
      "200/200 [==============================] - 0s 169us/step - loss: 0.3131 - acc: 0.8800 - val_loss: 0.4121 - val_acc: 0.8225\n",
      "Epoch 373/500\n",
      "200/200 [==============================] - 0s 194us/step - loss: 0.3136 - acc: 0.8800 - val_loss: 0.4132 - val_acc: 0.8225\n",
      "Epoch 374/500\n",
      "200/200 [==============================] - 0s 165us/step - loss: 0.3134 - acc: 0.8850 - val_loss: 0.4137 - val_acc: 0.8250\n",
      "Epoch 375/500\n",
      "200/200 [==============================] - 0s 175us/step - loss: 0.3136 - acc: 0.8850 - val_loss: 0.4126 - val_acc: 0.8275\n",
      "Epoch 376/500\n",
      "200/200 [==============================] - 0s 206us/step - loss: 0.3133 - acc: 0.8850 - val_loss: 0.4116 - val_acc: 0.8263\n",
      "Epoch 377/500\n",
      "200/200 [==============================] - 0s 191us/step - loss: 0.3128 - acc: 0.8850 - val_loss: 0.4118 - val_acc: 0.8237\n",
      "Epoch 378/500\n",
      "200/200 [==============================] - 0s 172us/step - loss: 0.3132 - acc: 0.8750 - val_loss: 0.4127 - val_acc: 0.8237\n",
      "Epoch 379/500\n",
      "200/200 [==============================] - 0s 192us/step - loss: 0.3131 - acc: 0.8750 - val_loss: 0.4119 - val_acc: 0.8250\n",
      "Epoch 380/500\n",
      "200/200 [==============================] - 0s 185us/step - loss: 0.3145 - acc: 0.8750 - val_loss: 0.4142 - val_acc: 0.8250\n",
      "Epoch 381/500\n",
      "200/200 [==============================] - 0s 165us/step - loss: 0.3141 - acc: 0.8800 - val_loss: 0.4145 - val_acc: 0.8275\n",
      "Epoch 382/500\n",
      "200/200 [==============================] - 0s 156us/step - loss: 0.3132 - acc: 0.8850 - val_loss: 0.4133 - val_acc: 0.8275\n",
      "Epoch 383/500\n",
      "200/200 [==============================] - 0s 176us/step - loss: 0.3133 - acc: 0.8850 - val_loss: 0.4124 - val_acc: 0.8250\n",
      "Epoch 384/500\n",
      "200/200 [==============================] - 0s 164us/step - loss: 0.3125 - acc: 0.8850 - val_loss: 0.4121 - val_acc: 0.8250\n",
      "Epoch 385/500\n",
      "200/200 [==============================] - 0s 162us/step - loss: 0.3128 - acc: 0.8800 - val_loss: 0.4107 - val_acc: 0.8250\n",
      "Epoch 386/500\n",
      "200/200 [==============================] - 0s 188us/step - loss: 0.3123 - acc: 0.8800 - val_loss: 0.4106 - val_acc: 0.8263\n",
      "Epoch 387/500\n",
      "200/200 [==============================] - 0s 177us/step - loss: 0.3118 - acc: 0.8850 - val_loss: 0.4096 - val_acc: 0.8300\n",
      "Epoch 388/500\n",
      "200/200 [==============================] - 0s 169us/step - loss: 0.3120 - acc: 0.8800 - val_loss: 0.4094 - val_acc: 0.8275\n",
      "Epoch 389/500\n",
      "200/200 [==============================] - 0s 210us/step - loss: 0.3116 - acc: 0.8750 - val_loss: 0.4103 - val_acc: 0.8275\n",
      "Epoch 390/500\n",
      "200/200 [==============================] - 0s 200us/step - loss: 0.3122 - acc: 0.8750 - val_loss: 0.4116 - val_acc: 0.8225\n",
      "Epoch 391/500\n",
      "200/200 [==============================] - 0s 170us/step - loss: 0.3118 - acc: 0.8750 - val_loss: 0.4119 - val_acc: 0.8225\n",
      "Epoch 392/500\n",
      "200/200 [==============================] - 0s 194us/step - loss: 0.3116 - acc: 0.8750 - val_loss: 0.4127 - val_acc: 0.8225\n",
      "Epoch 393/500\n",
      "200/200 [==============================] - 0s 181us/step - loss: 0.3117 - acc: 0.8850 - val_loss: 0.4131 - val_acc: 0.8250\n",
      "Epoch 394/500\n",
      "200/200 [==============================] - 0s 170us/step - loss: 0.3116 - acc: 0.8850 - val_loss: 0.4139 - val_acc: 0.8250\n",
      "Epoch 395/500\n",
      "200/200 [==============================] - 0s 167us/step - loss: 0.3114 - acc: 0.8850 - val_loss: 0.4145 - val_acc: 0.8250\n",
      "Epoch 396/500\n",
      "200/200 [==============================] - 0s 184us/step - loss: 0.3113 - acc: 0.8850 - val_loss: 0.4142 - val_acc: 0.8225\n",
      "Epoch 397/500\n",
      "200/200 [==============================] - 0s 166us/step - loss: 0.3112 - acc: 0.8750 - val_loss: 0.4140 - val_acc: 0.8213\n",
      "Epoch 398/500\n",
      "200/200 [==============================] - 0s 171us/step - loss: 0.3108 - acc: 0.8750 - val_loss: 0.4140 - val_acc: 0.8237\n",
      "Epoch 399/500\n",
      "200/200 [==============================] - 0s 180us/step - loss: 0.3105 - acc: 0.8900 - val_loss: 0.4153 - val_acc: 0.8237\n",
      "Epoch 400/500\n",
      "200/200 [==============================] - 0s 163us/step - loss: 0.3107 - acc: 0.8900 - val_loss: 0.4151 - val_acc: 0.8213\n",
      "Epoch 401/500\n",
      "200/200 [==============================] - 0s 118us/step - loss: 0.3107 - acc: 0.8800 - val_loss: 0.4158 - val_acc: 0.8225\n",
      "Epoch 402/500\n",
      "200/200 [==============================] - 0s 180us/step - loss: 0.3102 - acc: 0.8800 - val_loss: 0.4151 - val_acc: 0.8213\n",
      "Epoch 403/500\n",
      "200/200 [==============================] - 0s 165us/step - loss: 0.3112 - acc: 0.8800 - val_loss: 0.4144 - val_acc: 0.8200\n",
      "Epoch 404/500\n",
      "200/200 [==============================] - 0s 183us/step - loss: 0.3107 - acc: 0.8800 - val_loss: 0.4139 - val_acc: 0.8200\n",
      "Epoch 405/500\n",
      "200/200 [==============================] - 0s 205us/step - loss: 0.3100 - acc: 0.8800 - val_loss: 0.4140 - val_acc: 0.8213\n",
      "Epoch 406/500\n",
      "200/200 [==============================] - 0s 163us/step - loss: 0.3099 - acc: 0.8900 - val_loss: 0.4128 - val_acc: 0.8287\n",
      "Epoch 407/500\n",
      "200/200 [==============================] - 0s 161us/step - loss: 0.3111 - acc: 0.8900 - val_loss: 0.4137 - val_acc: 0.8287\n",
      "Epoch 408/500\n",
      "200/200 [==============================] - 0s 159us/step - loss: 0.3102 - acc: 0.8900 - val_loss: 0.4127 - val_acc: 0.8287\n",
      "Epoch 409/500\n",
      "200/200 [==============================] - 0s 189us/step - loss: 0.3104 - acc: 0.8900 - val_loss: 0.4131 - val_acc: 0.8287\n",
      "Epoch 410/500\n",
      "200/200 [==============================] - 0s 248us/step - loss: 0.3094 - acc: 0.8900 - val_loss: 0.4129 - val_acc: 0.8287\n",
      "Epoch 411/500\n",
      "200/200 [==============================] - 0s 183us/step - loss: 0.3111 - acc: 0.8800 - val_loss: 0.4146 - val_acc: 0.8237\n",
      "Epoch 412/500\n",
      "200/200 [==============================] - 0s 191us/step - loss: 0.3102 - acc: 0.8800 - val_loss: 0.4141 - val_acc: 0.8213\n",
      "Epoch 413/500\n",
      "200/200 [==============================] - 0s 165us/step - loss: 0.3097 - acc: 0.8800 - val_loss: 0.4147 - val_acc: 0.8237\n",
      "Epoch 414/500\n",
      "200/200 [==============================] - 0s 146us/step - loss: 0.3099 - acc: 0.8900 - val_loss: 0.4153 - val_acc: 0.8250\n",
      "Epoch 415/500\n",
      "200/200 [==============================] - 0s 160us/step - loss: 0.3091 - acc: 0.8900 - val_loss: 0.4157 - val_acc: 0.8225\n",
      "Epoch 416/500\n",
      "200/200 [==============================] - 0s 178us/step - loss: 0.3095 - acc: 0.8900 - val_loss: 0.4157 - val_acc: 0.8250\n",
      "Epoch 417/500\n",
      "200/200 [==============================] - 0s 195us/step - loss: 0.3084 - acc: 0.8900 - val_loss: 0.4133 - val_acc: 0.8237\n",
      "Epoch 418/500\n",
      "200/200 [==============================] - 0s 190us/step - loss: 0.3086 - acc: 0.8850 - val_loss: 0.4129 - val_acc: 0.8250\n",
      "Epoch 419/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 136us/step - loss: 0.3088 - acc: 0.8800 - val_loss: 0.4132 - val_acc: 0.8237\n",
      "Epoch 420/500\n",
      "200/200 [==============================] - 0s 205us/step - loss: 0.3089 - acc: 0.8800 - val_loss: 0.4149 - val_acc: 0.8225\n",
      "Epoch 421/500\n",
      "200/200 [==============================] - 0s 189us/step - loss: 0.3086 - acc: 0.8850 - val_loss: 0.4178 - val_acc: 0.8213\n",
      "Epoch 422/500\n",
      "200/200 [==============================] - 0s 126us/step - loss: 0.3094 - acc: 0.8850 - val_loss: 0.4187 - val_acc: 0.8225\n",
      "Epoch 423/500\n",
      "200/200 [==============================] - 0s 177us/step - loss: 0.3085 - acc: 0.8850 - val_loss: 0.4175 - val_acc: 0.8213\n",
      "Epoch 424/500\n",
      "200/200 [==============================] - 0s 166us/step - loss: 0.3082 - acc: 0.8850 - val_loss: 0.4168 - val_acc: 0.8213\n",
      "Epoch 425/500\n",
      "200/200 [==============================] - 0s 160us/step - loss: 0.3086 - acc: 0.8850 - val_loss: 0.4164 - val_acc: 0.8225\n",
      "Epoch 426/500\n",
      "200/200 [==============================] - 0s 154us/step - loss: 0.3083 - acc: 0.8850 - val_loss: 0.4171 - val_acc: 0.8225\n",
      "Epoch 427/500\n",
      "200/200 [==============================] - 0s 174us/step - loss: 0.3078 - acc: 0.8850 - val_loss: 0.4164 - val_acc: 0.8213\n",
      "Epoch 428/500\n",
      "200/200 [==============================] - 0s 149us/step - loss: 0.3080 - acc: 0.8850 - val_loss: 0.4161 - val_acc: 0.8200\n",
      "Epoch 429/500\n",
      "200/200 [==============================] - 0s 189us/step - loss: 0.3079 - acc: 0.8850 - val_loss: 0.4178 - val_acc: 0.8200\n",
      "Epoch 430/500\n",
      "200/200 [==============================] - 0s 138us/step - loss: 0.3079 - acc: 0.8850 - val_loss: 0.4175 - val_acc: 0.8213\n",
      "Epoch 431/500\n",
      "200/200 [==============================] - 0s 179us/step - loss: 0.3075 - acc: 0.8850 - val_loss: 0.4152 - val_acc: 0.8213\n",
      "Epoch 432/500\n",
      "200/200 [==============================] - 0s 159us/step - loss: 0.3079 - acc: 0.8850 - val_loss: 0.4146 - val_acc: 0.8213\n",
      "Epoch 433/500\n",
      "200/200 [==============================] - 0s 172us/step - loss: 0.3081 - acc: 0.8800 - val_loss: 0.4118 - val_acc: 0.8237\n",
      "Epoch 434/500\n",
      "200/200 [==============================] - 0s 178us/step - loss: 0.3076 - acc: 0.8800 - val_loss: 0.4122 - val_acc: 0.8237\n",
      "Epoch 435/500\n",
      "200/200 [==============================] - 0s 187us/step - loss: 0.3076 - acc: 0.8800 - val_loss: 0.4126 - val_acc: 0.8275\n",
      "Epoch 436/500\n",
      "200/200 [==============================] - 0s 171us/step - loss: 0.3068 - acc: 0.8850 - val_loss: 0.4136 - val_acc: 0.8250\n",
      "Epoch 437/500\n",
      "200/200 [==============================] - 0s 155us/step - loss: 0.3072 - acc: 0.8850 - val_loss: 0.4154 - val_acc: 0.8275\n",
      "Epoch 438/500\n",
      "200/200 [==============================] - 0s 167us/step - loss: 0.3066 - acc: 0.8900 - val_loss: 0.4153 - val_acc: 0.8213\n",
      "Epoch 439/500\n",
      "200/200 [==============================] - 0s 159us/step - loss: 0.3058 - acc: 0.8850 - val_loss: 0.4166 - val_acc: 0.8225\n",
      "Epoch 440/500\n",
      "200/200 [==============================] - 0s 200us/step - loss: 0.3078 - acc: 0.8850 - val_loss: 0.4185 - val_acc: 0.8213\n",
      "Epoch 441/500\n",
      "200/200 [==============================] - 0s 161us/step - loss: 0.3069 - acc: 0.8850 - val_loss: 0.4173 - val_acc: 0.8213\n",
      "Epoch 442/500\n",
      "200/200 [==============================] - 0s 162us/step - loss: 0.3069 - acc: 0.8850 - val_loss: 0.4177 - val_acc: 0.8187\n",
      "Epoch 443/500\n",
      "200/200 [==============================] - 0s 212us/step - loss: 0.3067 - acc: 0.8800 - val_loss: 0.4162 - val_acc: 0.8187\n",
      "Epoch 444/500\n",
      "200/200 [==============================] - 0s 176us/step - loss: 0.3070 - acc: 0.8850 - val_loss: 0.4156 - val_acc: 0.8200\n",
      "Epoch 445/500\n",
      "200/200 [==============================] - 0s 179us/step - loss: 0.3061 - acc: 0.8850 - val_loss: 0.4147 - val_acc: 0.8237\n",
      "Epoch 446/500\n",
      "200/200 [==============================] - 0s 160us/step - loss: 0.3074 - acc: 0.8850 - val_loss: 0.4132 - val_acc: 0.8263\n",
      "Epoch 447/500\n",
      "200/200 [==============================] - 0s 149us/step - loss: 0.3060 - acc: 0.8850 - val_loss: 0.4126 - val_acc: 0.8287\n",
      "Epoch 448/500\n",
      "200/200 [==============================] - 0s 158us/step - loss: 0.3059 - acc: 0.8800 - val_loss: 0.4118 - val_acc: 0.8325\n",
      "Epoch 449/500\n",
      "200/200 [==============================] - 0s 149us/step - loss: 0.3058 - acc: 0.8800 - val_loss: 0.4115 - val_acc: 0.8313\n",
      "Epoch 450/500\n",
      "200/200 [==============================] - 0s 154us/step - loss: 0.3054 - acc: 0.8800 - val_loss: 0.4125 - val_acc: 0.8263\n",
      "Epoch 451/500\n",
      "200/200 [==============================] - 0s 179us/step - loss: 0.3052 - acc: 0.8850 - val_loss: 0.4138 - val_acc: 0.8263\n",
      "Epoch 452/500\n",
      "200/200 [==============================] - 0s 176us/step - loss: 0.3053 - acc: 0.8850 - val_loss: 0.4153 - val_acc: 0.8250\n",
      "Epoch 453/500\n",
      "200/200 [==============================] - 0s 218us/step - loss: 0.3050 - acc: 0.8850 - val_loss: 0.4156 - val_acc: 0.8237\n",
      "Epoch 454/500\n",
      "200/200 [==============================] - 0s 175us/step - loss: 0.3046 - acc: 0.8850 - val_loss: 0.4150 - val_acc: 0.8263\n",
      "Epoch 455/500\n",
      "200/200 [==============================] - 0s 174us/step - loss: 0.3052 - acc: 0.8850 - val_loss: 0.4137 - val_acc: 0.8275\n",
      "Epoch 456/500\n",
      "200/200 [==============================] - 0s 206us/step - loss: 0.3053 - acc: 0.8850 - val_loss: 0.4123 - val_acc: 0.8325\n",
      "Epoch 457/500\n",
      "200/200 [==============================] - 0s 175us/step - loss: 0.3049 - acc: 0.8800 - val_loss: 0.4120 - val_acc: 0.8313\n",
      "Epoch 458/500\n",
      "200/200 [==============================] - 0s 203us/step - loss: 0.3051 - acc: 0.8850 - val_loss: 0.4125 - val_acc: 0.8300\n",
      "Epoch 459/500\n",
      "200/200 [==============================] - 0s 163us/step - loss: 0.3057 - acc: 0.8850 - val_loss: 0.4140 - val_acc: 0.8287\n",
      "Epoch 460/500\n",
      "200/200 [==============================] - 0s 234us/step - loss: 0.3049 - acc: 0.8900 - val_loss: 0.4173 - val_acc: 0.8225\n",
      "Epoch 461/500\n",
      "200/200 [==============================] - 0s 168us/step - loss: 0.3043 - acc: 0.8900 - val_loss: 0.4171 - val_acc: 0.8225\n",
      "Epoch 462/500\n",
      "200/200 [==============================] - 0s 178us/step - loss: 0.3057 - acc: 0.8900 - val_loss: 0.4184 - val_acc: 0.8250\n",
      "Epoch 463/500\n",
      "200/200 [==============================] - 0s 170us/step - loss: 0.3050 - acc: 0.8900 - val_loss: 0.4179 - val_acc: 0.8300\n",
      "Epoch 464/500\n",
      "200/200 [==============================] - 0s 176us/step - loss: 0.3049 - acc: 0.8900 - val_loss: 0.4192 - val_acc: 0.8263\n",
      "Epoch 465/500\n",
      "200/200 [==============================] - 0s 203us/step - loss: 0.3050 - acc: 0.8900 - val_loss: 0.4198 - val_acc: 0.8263\n",
      "Epoch 466/500\n",
      "200/200 [==============================] - 0s 140us/step - loss: 0.3044 - acc: 0.8900 - val_loss: 0.4191 - val_acc: 0.8237\n",
      "Epoch 467/500\n",
      "200/200 [==============================] - 0s 177us/step - loss: 0.3047 - acc: 0.8850 - val_loss: 0.4188 - val_acc: 0.8213\n",
      "Epoch 468/500\n",
      "200/200 [==============================] - 0s 162us/step - loss: 0.3036 - acc: 0.8850 - val_loss: 0.4168 - val_acc: 0.8263\n",
      "Epoch 469/500\n",
      "200/200 [==============================] - 0s 176us/step - loss: 0.3042 - acc: 0.8850 - val_loss: 0.4161 - val_acc: 0.8263\n",
      "Epoch 470/500\n",
      "200/200 [==============================] - 0s 173us/step - loss: 0.3036 - acc: 0.8800 - val_loss: 0.4166 - val_acc: 0.8263\n",
      "Epoch 471/500\n",
      "200/200 [==============================] - 0s 167us/step - loss: 0.3033 - acc: 0.8850 - val_loss: 0.4176 - val_acc: 0.8263\n",
      "Epoch 472/500\n",
      "200/200 [==============================] - 0s 161us/step - loss: 0.3035 - acc: 0.8850 - val_loss: 0.4188 - val_acc: 0.8200\n",
      "Epoch 473/500\n",
      "200/200 [==============================] - 0s 179us/step - loss: 0.3042 - acc: 0.8900 - val_loss: 0.4215 - val_acc: 0.8213\n",
      "Epoch 474/500\n",
      "200/200 [==============================] - 0s 184us/step - loss: 0.3044 - acc: 0.8850 - val_loss: 0.4210 - val_acc: 0.8200\n",
      "Epoch 475/500\n",
      "200/200 [==============================] - 0s 165us/step - loss: 0.3037 - acc: 0.8800 - val_loss: 0.4210 - val_acc: 0.8213\n",
      "Epoch 476/500\n",
      "200/200 [==============================] - 0s 157us/step - loss: 0.3029 - acc: 0.8850 - val_loss: 0.4214 - val_acc: 0.8213\n",
      "Epoch 477/500\n",
      "200/200 [==============================] - 0s 213us/step - loss: 0.3027 - acc: 0.8850 - val_loss: 0.4218 - val_acc: 0.8237\n",
      "Epoch 478/500\n",
      "200/200 [==============================] - 0s 209us/step - loss: 0.3029 - acc: 0.8850 - val_loss: 0.4205 - val_acc: 0.8275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 479/500\n",
      "200/200 [==============================] - 0s 178us/step - loss: 0.3029 - acc: 0.8900 - val_loss: 0.4211 - val_acc: 0.8275\n",
      "Epoch 480/500\n",
      "200/200 [==============================] - 0s 265us/step - loss: 0.3038 - acc: 0.8800 - val_loss: 0.4188 - val_acc: 0.8237\n",
      "Epoch 481/500\n",
      "200/200 [==============================] - 0s 188us/step - loss: 0.3037 - acc: 0.8800 - val_loss: 0.4187 - val_acc: 0.8263\n",
      "Epoch 482/500\n",
      "200/200 [==============================] - 0s 185us/step - loss: 0.3028 - acc: 0.8800 - val_loss: 0.4187 - val_acc: 0.8263\n",
      "Epoch 483/500\n",
      "200/200 [==============================] - 0s 218us/step - loss: 0.3026 - acc: 0.8800 - val_loss: 0.4185 - val_acc: 0.8250\n",
      "Epoch 484/500\n",
      "200/200 [==============================] - 0s 152us/step - loss: 0.3023 - acc: 0.8800 - val_loss: 0.4196 - val_acc: 0.8263\n",
      "Epoch 485/500\n",
      "200/200 [==============================] - 0s 140us/step - loss: 0.3019 - acc: 0.8900 - val_loss: 0.4231 - val_acc: 0.8237\n",
      "Epoch 486/500\n",
      "200/200 [==============================] - 0s 181us/step - loss: 0.3026 - acc: 0.8900 - val_loss: 0.4244 - val_acc: 0.8225\n",
      "Epoch 487/500\n",
      "200/200 [==============================] - 0s 163us/step - loss: 0.3025 - acc: 0.8850 - val_loss: 0.4236 - val_acc: 0.8250\n",
      "Epoch 488/500\n",
      "200/200 [==============================] - 0s 178us/step - loss: 0.3019 - acc: 0.8850 - val_loss: 0.4225 - val_acc: 0.8250\n",
      "Epoch 489/500\n",
      "200/200 [==============================] - 0s 195us/step - loss: 0.3015 - acc: 0.8900 - val_loss: 0.4225 - val_acc: 0.8287\n",
      "Epoch 490/500\n",
      "200/200 [==============================] - 0s 174us/step - loss: 0.3030 - acc: 0.8900 - val_loss: 0.4230 - val_acc: 0.8287\n",
      "Epoch 491/500\n",
      "200/200 [==============================] - 0s 158us/step - loss: 0.3027 - acc: 0.8900 - val_loss: 0.4219 - val_acc: 0.8287\n",
      "Epoch 492/500\n",
      "200/200 [==============================] - 0s 155us/step - loss: 0.3025 - acc: 0.8900 - val_loss: 0.4201 - val_acc: 0.8275\n",
      "Epoch 493/500\n",
      "200/200 [==============================] - 0s 222us/step - loss: 0.3019 - acc: 0.8850 - val_loss: 0.4223 - val_acc: 0.8225\n",
      "Epoch 494/500\n",
      "200/200 [==============================] - 0s 254us/step - loss: 0.3015 - acc: 0.8850 - val_loss: 0.4231 - val_acc: 0.8250\n",
      "Epoch 495/500\n",
      "200/200 [==============================] - 0s 198us/step - loss: 0.3022 - acc: 0.8900 - val_loss: 0.4250 - val_acc: 0.8250\n",
      "Epoch 496/500\n",
      "200/200 [==============================] - 0s 246us/step - loss: 0.3015 - acc: 0.8900 - val_loss: 0.4264 - val_acc: 0.8237\n",
      "Epoch 497/500\n",
      "200/200 [==============================] - 0s 219us/step - loss: 0.3019 - acc: 0.8900 - val_loss: 0.4264 - val_acc: 0.8237\n",
      "Epoch 498/500\n",
      "200/200 [==============================] - 0s 212us/step - loss: 0.3018 - acc: 0.8850 - val_loss: 0.4246 - val_acc: 0.8237\n",
      "Epoch 499/500\n",
      "200/200 [==============================] - 0s 199us/step - loss: 0.3013 - acc: 0.8850 - val_loss: 0.4239 - val_acc: 0.8263\n",
      "Epoch 500/500\n",
      "200/200 [==============================] - 0s 239us/step - loss: 0.3014 - acc: 0.8850 - val_loss: 0.4237 - val_acc: 0.8263\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.885, Test: 0.826\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd8VFX+//HXmUnvkIRACiQgXXqRpmCnqIi9l9VFXV3b\n14K7rrvu7m+XXV3Xhou6i+7aULGhghQpooA06SQk9ISQBul1Zs7vjzNJJo1ESDLM5PN8PPLIzJ07\n954zk7znzLnnnqu01gghhPAuFncXQAghROuTcBdCCC8k4S6EEF5Iwl0IIbyQhLsQQnghCXchhPBC\nEu5CCOGFJNyFEMILSbgLIYQX8nHXjqOionRiYqK7di+EEB5p8+bNuVrr6ObWc1u4JyYmsmnTJnft\nXgghPJJS6lBL1pNuGSGE8EItCnel1GSlVIpSKk0pNauRxzsppT5TSm1XSm1QSp3d+kUVQgjRUs2G\nu1LKCswBpgADgBuVUgPqrfYbYKvWejBwG/BSaxdUCCFEy7Wk5T4aSNNa79daVwLzgen11hkArADQ\nWicDiUqpmFYtqRBCiBZrSbjHAUdc7qc7l7naBlwFoJQaDfQA4lujgEIIIX6+1jqgOhuIUEptBX4N\n/ATY66+klJqplNqklNqUk5PTSrsWQghRX0uGQmYACS73453LamitC4E7AZRSCjgA7K+/Ia31G8Ab\nACNHjpRLQAkhRBtpSbhvBHorpZIwoX4DcJPrCkqpCKDU2Sd/N/CdM/CFcIuM/DI+3nSEyWd3pV/X\nsCbX23zoBKtTsgG4fEgsvWNCm93urowCLhnYFYAVyVlsPZzPsB6dOPesKD7alM5Vw+MI8LU2eO6a\n1BxiIwLpFR0CQMqxIr7efhQAf18rt49LJMTfh8N5pXyyJZ1gfyt3jk/C11r3C3ZqVhFf78jkqmHx\nLN+TRWSIH9OHxnGsoJx1+3MprbRz7YgE/Hya/2KutebzrRmc1zuayBB/TpRUsjIlmxnD4jDttIa+\n25tD1/AA+jTzWjVnf04xC7cdZcawOHpEBlNSYePttQepqDJf+qcM6kZUiD8fbDiMze5gQGwYk8/u\n1uLtbzhwHD8fC0MTIlr8nPzSSv637hA2u6Nm2eikSCb0jgIgt7iC7/bm1Lw+Gw8ex89qYUhCBFpr\n/rfuECH+PpzVJYRv92QBcMnAruzOLCT9eCmT+nVhePdOLS7P6Wg23LXWNqXUA8ASwArM01rvUkrd\n63x8LtAf+K9SSgO7gLvasMxCNOvFZXv5eHM6ezILef3WkU2u96evdrP1SD4A+3JLmHPT8JNu9/rX\n15F+oow9f5xMgK+F//toGydKq4gK8eeJyX35zWc7yCwo4/8u6VvneVprbv3PBgAOzp5myrh8L4t3\nHqtZp0uoP9eOTOC5pSl8uc2EflxEENMG1w20P361mzWpuXy3N4cth03Zz0mK5NGPtrJ2Xx4Adofm\ntrGJzb1MbD2SzyMfbmP60FheumEYf1m0h483pxPfKYjRSZ0brK+15rZ5detxqt5cs58PNhwhp6iC\n/zdjEEt3H+O5JSk1j+/OLGJoQjgvLNsLgK9Vsem3UYQH+Ta7bZvdwXWvrwPgwF+nNvlBVd9bPxzk\npW9TqV5da4gMPsyPv7kQH6uFRz7cyprUXHp3CWVAbBjXzjX7ODh7Gnuzivn9wl0AhPr7UFRhA+Dt\ntQcpLDe3F+08xrJHzmtxeU5Hi85Q1VovAhbVWzbX5fY6oE/rFk2cKbTWPPXpDkor7Uwb3I131x9i\n5nk9Obd3NBn5ZTz92Q7O6RnJvRN7tcr+VqVk8+aa/YQF+PL8tUMI9vehyu7giQXbySos58bR3enb\nNZRXV6RRWmmjV3QIT03tX/P8SpuDb3aZ0NybVdzkfhwOTWpWEXeMSyT9RBl7jxU1WZ5NB0/w2KV9\nST9RBkBadjExYf6cKK0ioXMgR46X8cSC7QC8siKNcb2iGNsrkvQTpbywbG+d12bbkXyeX5rCmtRc\nJg/sypybhzPgmW/Ym1XEO+sO8uW2o9w4OoHle7JZuC2Dg3kl9IwKZsqgbsxZmcaa1FyAmmAH+HpH\nJrnFFTX3n/liF4PjIxpttX648TCVNge3jk1k0Y5MAI6XVAJwotT8fmLBNmIjAgnyMxFRWmkjNMCH\nxy+t/dCavTiZsEAfvk/NxaIUD13Um1GJnfnjl7tJPlbIhf1jKCqvYmBsOBcPiCG/tJLHPt5OaaWN\n6FB/sgsrnO+Red1TjhXjZ7Ww64+X8vCHW9mRXkCgn5WEzoG8euNwps/5gU9/SmfjwePkl1Y1+l51\nDQ/gb1cP5hdvb6xZdv3r6wkJMPUor7IT7O/D89cM4e21B/nxQF7NeuGBvizeeYyxPSP5YOYYAL7Z\nmcm9727hutfXERMWUPPaPzj/JzoH+9U8V2tN8rHazoqiChu/v3wAFTYHsxcnA/DrC87ilRVpXP/G\nemYMi+PG0d0brUNrcdv0A8JzbDl8gvkbzYCphc4WZaXNwbm9o5m/4TArU3JYmZLDzed0JzSg+VZV\nc176NpXUrGKKK2xc1D+Gq0fE831aLp/9lEGIvw8Z+SmUVNhrwmz5nmxuPqcH3SODABMWReU2ekUH\nsz+3hPIqe6PdJBn5ZZRU2unbNZRgfyurUrKpsNnx96m77h1vmaC4ZkTtALCUrCLyy0wQXj44ltdW\n7QPg/L7RrEzJ4bVVaYztFclfFyfz9fZMClzC6JUVtQHdJyYEq0XROyaEnRmFfLQp3exzXBL+Plbe\nXnuQJbvM1/sDf53KWz8cBGDCWVF8n5aLv4+FpKhgvtx2lMIy0zq8qH8My/dk8eaa/Q2+iVTaHDz5\nyQ4Arh2ZwKZDJwDIKix3vibmd3SoP8dLKmu+CfSJCWHtvjwsLi3OuatNnbt3DuJYYTmfbkmnU5Av\n8344QIi/T81zwbRs1+7LY/meLBIjg+o8lnKsCK01e7OK6BkdjK/VQt+YUL7enolG0zcmlMHx4fSI\nDOLZL3cDMCQhAj9r3dZvSYWdtfvySIoMZk1qLtGh/vSNCaWowsaGZNP1dnZcGGv35THvhwO8siKV\n7p2DiA715/DxUrKcHzb3Tar9IJ7UtwtTzu7KhgPHaz5MwwN9OZBbQkFZ7XuakV/G3qwifCyKW8b0\n4EBuCdOHxmF3aNbuy2NAtzDuntCTnRkFFFfYsDva/pCj0to9xzVHjhypZW6ZM9OuowXc+MZ6quya\nsio7Vov5J6r+gwzys1Jaaeeq4XFsPnSCnKIKSivt+FoVVXZdp693aHwEH94zptGvof9YmkJadjHl\nVXYGxUewdNcx9ueWUGlz8MTkvry3/jCZBWX0iQmlb9dQViZn8/ilffndF7sabMvHorA4y1lpM/2l\nj17chxeW7SXA10KFzcF9E3vxxOR+PPrhVr7akYnWmiq75pP7xpJ+ooyH5m/Fz2qBekWt3p6PRWFz\nvgZWi0IBNodm8UPnMuWlNSRFBbPysUk8tySZOSv34edjqXluU/585dncMqYHj360lU+3mHEK8+4Y\nyQX9Ythy+ARXvba2wXOentYfi1L88avdnNUlhGtGxNe0DmdN6ce9E3vxu8938s76QwT6Wimrste+\nJxoqnf3J1e9Xtery3nNeT56a2p+M/DLGz14BwA+zLuDW//zI/pwSAN696xxu+c+PAHxx/3j+smgP\nVXbzgf/yilRevH4oD83fWrPtZY+cx1fbM3llRSoLH5jAZa98b/ZptVBpd9T8ru4e+mbnMe59dzNg\nwvbJyf14fkkKr65MIzY8gB9mXdDgb6q8ys7IPy+nuMKGn4+FTU9fRFiAL1prkp4yHQ/7/jKVKS99\nV/ONbsnD59G3aygfbTrCEwu2c8/Enjw1pT/1vfJtKv9Ytpc/XD4AH6uFpz/fyYMX9ua83lFcM3cd\nvlaF3aHpFR3CskcnnvQ9P11Kqc1a66b7Gp2k5d7BZOSXcby4ssHymDB/uoQFUGV38PySFCpsDiqc\nwTSxTzTXjIgnyM/KlsP5XDM8ntvf2lATRn+7ehAlFXY+2nSE5GNF9IoOYWKfaA7llbB45zG+3pHJ\neX2iKa+yk1VgWkcOrXllRVrN/lemmKGxF/WPYUBsGDef04NhCZ34eNMRPv0pg+RjRVw/MoFrRyZw\norSKkgobr39nBmT9/ZrBNaFjdzh4c80BAO4cn4jW8M/lps/2tVX7OLd3NF9sO8qYnp0ZFBdBeKAv\nQxM60a9rGI9c1IeyqgYjePHzsRAR6Et2UQV+VkV0qH9NC7d75yD6dwvj+WuHMOGsKOd+k7AoE5xK\nQVxEIOknyuga5k9BmY0qu4NrR8azKiWn5tvAryb1IiYsgPBAXyb26QLAsIQIfn/5AOwOTUmFvaYe\nfbuGMjA2nPzSSsadFUX/bmGUVtrRWnP1cOf2zu9FeKAvr640r/Flg7vRJTQAgNAAH6wWxZyVaVTZ\nbdx/fi/nawdWCzXdBbHhATWvQayzu+PbPdn0iAxi/FmR/Gn6QOwOzeD4cPp2DeXTLRnkl1YxJimS\nywfHklNUQVmlKff/1h3iYF4JiZHB9O1aeyD2nok9UUpRaXOgFMwYZk6hmdQ3mkcv7kOFzc5N5/So\neT8tCsb0imy0sRDga+Wf1w9l86ETDIoLJ8z5LVIpxUf3jMXu0Fgtir9eNYhlu7OJ6xRYU5YZw+Io\nrbBx7ciEBtsF+OV5PQkN8OHGc7qjUNjsDq4f1R0/HwtPTu5X04o/13ng9UwgLfcOJK+4gnGzV9SE\ntquIIF/WP3Uhr61M4+UVaUwb1I0Qfx8+3HSEH2ZdQFxEYJ31dx0tYNrLpvW17ZlLCA/y5YutGTw0\nfysv3ziMK4bEkl9ayei/fEulzcHw7hEczCut6dttyuf3j6/TT1xhszPoD0uptDl47+5zGH9W7T9P\n4qyvCfC1kPynKXW2kTjra6D2gN9vP9vBez8errNOdYvNkzz+8TY+3pzOht9eWBPUzZn84nckHyti\n75+nNBg985/vD/Cnr3bz0T1jGz14Cg1fy6Z8tPEIT3xijjnMvmoQN7j0J9/6nx9ruqGmDe7GnJuG\nc/7zqziQW8K/bxvJRQPkZPafo6Utdwl3D5SWXcyaVNPSPa9PdM3Quvq+T80ls6AMf18recUVZJwo\n49/fH2D2VYOICvGvWW9/bjF/WZTMHeMSWbLrGBal+OrXEwjyt3Iwt7TJENx86DhBfj7072aGGmqt\n2XW0kIGxYTUtq91HC3ln/UE+2GD67J+c3I/eXUx5g/ythPj7sGBzOv9bZ2Yx3fXspQT71/1CmZZd\nRE5RJWN6dq7TYjtWUI6vVRHpUhcww9Vsdk1XZ8uz0uZgf24xJ0pMi79TsB8jerTPcLTWVGV3sC+n\n+KRDO+srKK2ioKyq5niEq+r36+y48Cafn11kvqE092FSaXOwbn8eChjXKxIfl+GbOUUVbHOOSBrW\nPYLIEH/ST5RyMLeUsb0ia7r9RMtIuHux2+dtYPVeE+6jEzvz0b1jG6xTWF7F4D8sbbC8d5cQltYb\nimWzOzj/H6s4ctyMBJlz0/AGw+9Ox5HjpVz4wmqigv1Y/cT5DcZt78sp5sJ/rKZf11C+efi8Vtuv\nEN5I+ty9WPKxQi4fEktiZBCvrEjj/ve2NDgImFtUUef+ZYO78dX2TM7v16VBf6WP1cK3j06itNKG\n1aJaZcSLq4TOQWx75hKsFtUg2AF6RYew+4+XNvqYEOLUSLh7mPzSSrIKKzg7NozLh8SyKiWnzvha\nV5cOjGFUYmdOlFZy4+ju5JdWcef4xEbX9fOx4Ofj1+hjrSHQr+FQRFfV46mFEK1D/qPcbPnuLJ75\nYieBflZGJ0Xy16sG1Tz2h4W7WLwzE4eG+E6BXDsioWbERJ+YUGIjAvny1xNavK937z6n1csvhDgz\nSbi3s+o5K3ysFrILy/ly+1GOFpiDVvtySrhtbA96dwnheGkl7/14iMhgf3KKyskpqmB7egFdQv25\ne0ISY3tFurMaQogznIR7O7vohdVYLYqnpw3gTpdTpKtNeWkNw7tH1JwN9/y1Q2pOFrE7NLeO7cGv\nJp3VrmUWQngeOYLVjrTWHMwrZV9OCQs2pzd4vHquCtc5Q8b2imThA+PNmZOYU92FEKI5Eu7tKNfl\nzNCvd2Ryft9owIxkAXjzthH0qzem3GpRDI6P4OIBMYzrFUlC54bjlYUQoj4Z595GFu3IJCbMn40H\nT5B+opSETkFsPZJfZ4rXj+8dS/9uYQT7WSmptBPi70NJhY3dmYV1phIFc5KIQ+tGJ8ASQnQcMs7d\njbTW/Oq9LQ2WWy2KLqH+KAWJkcGM6N6pZrKrEOdZmcH+Pozs0Ylzkjpz+7jEmue25MILQghRTcK9\nFR3MLeGhD7eSmV9WZ/m0Qd1qumH+ffuoZrejlOLDexqedSqEEC0l4X6abHYze2Kgr5l7e1dGQc20\nsF3DArhrQhIWi+LrHZmEB7bdSUJCCOFKwv002OwOLvjHag4fL61Zdn7faLKLKth1tJBFD51L52A/\nljivCpQUJQdDvcKmt6DrYIgf4e6SiFNRVQ5o8HWZ6bSyFCw+0IZnabc3CffT8MO+PA4fL6Vf11CS\nnZdo+8MVAwkL8OWnIydqhjZeMiCGOTcN55KBMrVpu7BVwpcPQnEWjLob+p3etT7ryN4DXz0MVn/4\nXXbrbbe9HN8PITHgF3zy9TK2gH8YRJ3mORWFRyGwU90gdbeP74BDa+GOr6DbYLPsv5dBcQ5c/SZ0\nHwOVJebvp3NPtxb1dMhRutPw7Z4sgv2sPH/tEAB+M7UfPSKD6RTsxwX9aoNcKcW0wd1kYqzTVZwN\nqcvMVYtPZt+3sO0DOPg9/Pj6ydfVGg58Z1puzSkvgPevN7ftFfDTezD/ZvNhsm8lzJ0AXz8Ge5dC\nWf7JtwWQtw/emQHLnoEsc/k4cvbCsR2Nr79/Nez5Et6aCqufM/suO9H8fqrrufMTeHkYvHtN3cdS\nFpttvToa5pwD798Ab55vfqrKYPcXkPZt7et++EfIN1M447DD3iUmDAFK8uCnd6EkFyqK4IX+8Fxv\n82HRktdj7rmmHK+Ohk3zWla3+opz4PB6SFve8LH0zbB3MVQUwPrXzLLMbZCxGUpy4K0p5jX+7B7z\nWhWf5AO8otjUNSel4WNrXzF1ePsy8566gQyF/JnSsovJLCjj3N7RXPf6OhwOzYL7xpGRX0ZseEC7\nXNXcLU4cNME67JaGrTCHA3Z8DI4q09oZcz/4NjL/t8PhDMV3IbQbLP89jHsQht1qHrc4P/wqikyI\nDroGlAV6XWC6Qr5/wTw+6pcw7Xk4fgDeudIEyuS/mH+2VX81geQfCn2nwNb3IG4E9L8CfAJg+G1w\nYDV89QiEx8Pg62HRY9D7Epg0ywR4rwtcymwHi3P46ZZ3YOEDMOU5+OZJ0M6Lngy8CvavhPJC0M4r\nOUWeBV0GmMC64hUIi4XD6+Dsq8zjlaXw+rmQ57walcUHLv0LrP47lB2Hkb+A6H6mLNs/hNLjsPHN\nxt+bC56G8Q/Dlv/B5rehKBMu+B0MuAJ2fQZDbjL7fufK2uc8vAMOrTPl/fw+syyiO+TXvagJ/S83\nYQfm24q9onbdmatNAK/4E0T3h/MeMx9UhRnmfnhcbcAGd4EHfzLlqyw2f0eBncHqa34Avn/R/E30\nvwKObTev/W1fmNfW4YCKQugzGbqe3fjrUP1+/fNsKDLX+uVX66GL87J5VWXw+kSznfhRsGehqV/G\nT+Y1v/d782HbOQn2r3K+L74w/kH48Q3zfp91IVz/jnls2e/hhxfNbb8Qs+++k6HfZfDJXeb9yz9i\n/l9G3Q19LjUNj+9fhDG/ggt+23Q9TkLmc28j1VemOfDXqQz70zKmDurGX2YMauZZZ5iKIhMmJ/uq\nXFUG9ioICDOt0JeHmX+ALgPg5gVmeeoyEwZ+wSYgq02fY/55XTns8NpYyG2klQPmn+y6d0ApWPgg\nbPlv7WO+QVBVCr7BkDDa/LP3mWJCLHOry0YU9BgHscMg8VzIS4WlT9fdz6TfwKb/mA+hptz9LQRE\nmG1/chckTTThnPatqeuDP8GnM2HHR87dWkyYXf8OFB2Dt6fW3Z7FBxy22nr2vtS0LLe+a8Irogd8\ncCPk7DEh0XMSJH/VeNl8AuC+teY12Pq+aXG6CuwMnRLhqEtLedJvzPu09V0ICDfvv653Na5fb4FO\nSZB/yLTkh90Cb0yC4/vMazFpFhzdCtvn1z7HP8wEpbLWfqj5BJhwTlkM9koY92vzQfDlg9BlIGQ3\nvP4tw241H0ZfPWxawQ9ugW3zTeu5PmWBvlMh5mwI6gxZO2HEHeb1z0uFz+6ruw9lgVs/N38Pq/8G\nq2fDLZ9CZC/4/p/m25dScMciSBgFi56ADc5vewOuNN9KbGUQ3h06J5pveb9abxoB715tPqig9m/U\n1a9+NN+Wvvt73eXdhsL5vzFhfwok3NtIdbivfnwSE59bxbNXDKwzHv2Mt/sLWPhrSBgDN3/U+DoF\n6TBvMhQcMX/gUb3hu+dMSPzwEvj4m3Aod3Y9uLboALqPg/iRpn/aVm7+CbsOghV/No/PeB12fW5a\nQZUlprUGJtx9/OH96yDpPCjIgIiE2lbU9Ndg0LWw5CnT9QEmhHqdb4Igdjhc9k/zwQMm0ObfbD6k\nel0Auz41HwgAV7xqWuFhcSas35kBh35o+nUL726Ccepz0GOs6Xb46HbT+uoxru663z1vWrN3fG0+\nDF+fCAXOFnFwtPn6DzDhUbjIWffCTBPoZ11o+nlTl5uW+t5vzHr+oaa/fPqrdfelNSz7nekGOPtq\nuPo/Ztn8m0z3g6tht5gP3uXPwu7PzTeg3BQTTJP/2rDOpcdNyz9uBMQONdtNWw4J55jQ//ox835d\n/y4c/cm8JmdfDdZ6h/KqyuF/003/+6hfmJB//9q66/iFQmWR+WZ1xStgq4CXhpoWeNJECOkCZ11s\nylO/XhYf86FbXuB8XR8xf6tzx0Ous0skbiTkpkLSuXDDe7XPzU42f8fdx5j7GZvhk1+a4zSX/AmS\nF8H8G+GiZ6HHePjPRbXP9Q2Ce9aYb7WJ480H26rZkLEJrvufKVP+YVjwCxPmefvMOoOubfybbQtJ\nuLeR6nC/57yevP7dfr58YAKD4pu+TNkZ5eD3pg8wMML01QZFmVbegOmmZVpVagK5ui/SVb/LzD/F\njgXw2b2m22Hi47Dh33B4LUx93vyDHPgOlvym8f0HhMOjyeBXb9SQww6vjDAtvfJC51f+lSY4qsu9\n5R2Y8jdT9lOVk2KCNroP/GKJ+acMizXl2vmJ+Se85M/mNcncbroyhtxoWtLhcS3fj73KtChjh5n7\nxTkmNHqdb+q66DHzYTPompNvR2sTml0H1XZdNMZhN+vFDq/t2rJVmi6f4/vhw5vNMtcuCnc7fsB8\ncyzLN+H6w0vmW8ulf4EQMy0HZSdMYHdKrH2e1nBkg+kqqyw2o5aW/8F0BQ28CmIGmg8iMI2DshPw\n479MV2BgZ7j9y5N36zQmY4vZrsXHvHfHdpowH3ar+QbQziTc28B9726uM31AYmQQKx+b5P5+dlsF\nWP3M10utYcGdphXVfSys/5f5yjz0ZvMV0+oDd35jWpa7Pjetpcaccy+c+3+mNZm1CybOqv2nKzpm\nRkD4+Jvb616FSU+ZlorDblr5XQeZr/H9L4PQWChMh5CuENbE5ftSl5tRDErBnYvM89tCSa4J88bC\nsjCz6fJ5utLj5htUR5WTYkYJnU7j4Awh4d4Gqlvt1T6cOYZzerpxXnWtTb/r4idMn+J1/zV9nR/f\nbsLeXmkOQCkLpCwyz7lrmem3BtO6UxYozTVf/8PjIW+/ackHR9e2AttLVZmpU/2WvRCiRqvOLaOU\nmgy8BFiBf2utZ9d7PBx4F+ju3ObzWuu3fnapz2CllbY698f1inRfsG/7EDa/ZQ6MZe00/ZV7F5th\nbMXZpqX8i29MH+NZF5r+8YUPmtZwdbBD7QkboV3NQSkAd04VfyaNhRbCwzUb7kopKzAHuBhIBzYq\npRZqrXe7rHY/sFtrfblSKhpIUUq9p7WubGSTHulovrla0j3n9eTakQnEd2rHIKoqMy3r/tPN/S8f\nNAcqI8+Ci/8IYx+AdXNMF0xVCYz+JXTqYX7AjGa4ck77lVcI4XYtabmPBtK01vsBlFLzgemAa7hr\nIFSZzucQ4Dhgq78hT5ZZYCYDu6BfF87qEtI2O9m7xIxAGHQNTHgYfALNULPVf68dnlVt8mwYPbN2\nDPb4B00/+fF9JvSFEB1aS8I9Djjicj8dqH+l5VeBhcBRIBS4Xuv6A2k9W6az5R4bcYot9oPfm6Ps\nY+41/co/vGhOpEh0ucD1jgVmyNz3L5gx5CXZDcdjR/eDbkNg5F21wV7Nx+/MGQ0hhHCr1ppb5lJg\nK3AB0AtYppRao7UudF1JKTUTmAnQvXv3Vtp1+1i/P4+wAB+6hZ/C+NTiHHjbOb9Jj3GQuqR2zPcv\nV0LccHOm346PzAkafSabLpbOPc1428JM05IPjnbL0CshhOdpSbhnAAku9+Ody1zdCczWZuhNmlLq\nANAP2OC6ktb6DeANMKNlTrXQ7Sktu5hfvL2RjPwyrhkej8+pzA/z479qb789zXS1dB1kTpt/a6pp\nbVefUdjnUhhxu/kRQohT1JKk2gj0VkolKaX8gBswXTCuDgMXAiilYoC+wP7WLGh7qLQ5qLQ5am4f\nOV7Kf9ceJLOgjBtGJXDvpFNsNW//2LTG71trTtDpdYGZl+O2L8zY8OP7zUHRR3bDcAl1IcTpa7bl\nrrW2KaUeAJZghkLO01rvUkrd63x8LvAn4G2l1A5AAU9qrXPbsNxt4opXv8ffx8IXD0zg/ve3sGy3\n6e++sF8X/t+pzh9TeNT0o4+5z5zldu/3ZrlS5kzJ/0sGVMNTtoUQ4jS0KFG01ouARfWWzXW5fRS4\npHWL1j601izbnUVucWXNnOzv/XiIFcnZTBvUjfP7dWHCWVGnvoNDa83v7s5j0PXPZj3ZaeVCCHGK\nOnxzcdXeHGa+U3dmvd9+thOLgocu6k2fmNDmN5J/GD68xcxzsfcbM53ruY+Y6UKX/96c9tx1SBvV\nQAghGuqh3V7XAAAXeElEQVSQ4b5gczrJmYU8fHEfvtx6lLAAHz67fzzhgb5oDSUVNoL8rXQJbeHI\nmEWPmwn/M7fVLqseDQNw/m+l20UI0a46XOIUllfx2McmhLuGB7Bk1zGmDe5Gr+jaE5OiQ/1bvsG0\nb01rfewDZgItq585UPrjXDNT380LILpva1dDCCFOqsOF+9JdtScF/XPZXkoq7Vwx5GdM5+rKboOP\n74TwBHMVmqE3mS6Y4KiGc3wLIUQ76nAX9Vy47SjxnQJ5+KLelFTaiQrxY0zPnzEVqtZm+tQt70BO\nsrkW4wVPmylwYwaaYBdCCDfrUC33wvIqfkjL5Zfn9uSKIbG8uDyVqYO6tfzEpA1vmqvYdEqELJeL\nGFdflEEIIc4QHSrcU44VYXdozunZmZ7RIcy7YyTDEjo1/8TS4+YqRSv+ZC5u4RrsKJmoSwhxxulw\n4Q7Q1zm88YJ+MS174pzRtde9jB1mroM46Slzkd6yEw0n8BJCCDfrUOG+N6uIUP8mJv/aNh9K88x1\nEnd+Yi4x99WjZpmtrHa9C58x0wdUC3bjlZiEEKIJHSbctdas25fHwLiwhtc8LS+Az+6pu+z96yCy\nN4y6y5xVuvYVs7zLwPYpsBBCnIYOE+6PL9hOanYxt41tJJwzt9e9HxJjhjdeM6/2akZjH4DcvRDa\nwq4cIYRwow4R7ofzSlmwOZ2oEP+GY9rTN5u51AHuWQNBkRDeyLj30K7mRwghPECHCPcvtx8F4PP7\nxxEe6AO2SnPVIrsNPrjBXPGoywDoNtjNJRVCiNbRIU5i+nLbUUb06ER8pyBY8zz8NQ4qiuDgGhPs\nV/7LzK8uhBBewuvDfW9WEcnHirhiSKxpsa/4M9gr4af3YPGT5szSgTNMS14IIbyEV3fLrEjOYv6G\nI1gUTBnUFbJ21j74zZPm96SnwPcUL3othBBnKK9tuecUVXD3fzexdHcWF/aPMdP3Vk/JO/V5COwM\nl78M5z3u3oIKIUQb8NqW++KdmTg0fH7/eAbHhZuFmdsgIBxG3Q0j7wKL1362CSE6OK8N9y+3HaVv\nTChDEyJqFx76AeJGmpOS6p/IJIQQXsQrm65ZheVsPHiCy4d0q12Ym2pOQnKdOkAIIbyUV4Z7alYx\nACN6OOdpryiC9683V0nqO8WNJRNCiPbhld0yRwvMRF9xEc5RMEt+AycOwK2fQ2QvN5ZMCCHah1e2\n3DPzywGICfeHlG9gy/9g/EPQc6KbSyaEEO3DK8P9aH4Z0aH++PtYYfVsiOprxrMLIUQH4Z3hXlBG\nbHgAlOWb4Y8DZ4CPv7uLJYQQ7cbrwr3CZmfbkXx6x4TC4XWgHZA4wd3FEkKIduV14f5DWi6F5TZm\n9PaB718Eqz/Ej3J3sYQQol21KNyVUpOVUilKqTSl1KxGHn9cKbXV+bNTKWVXSnVu/eI2r3oY5Jgt\nj8OR9RAcDb6NXFZPCCG8WLPhrpSyAnOAKcAA4Eal1ADXdbTWz2mth2qthwJPAau11sfbosDNySup\npLNvJdbDP5gF5z7ijmIIIYRbtWSc+2ggTWu9H0ApNR+YDuxuYv0bgQ9ap3g/X25xBRMDDkAVcOtn\nckaqEKJDakm3TBxwxOV+unNZA0qpIGAy8EkTj89USm1SSm3Kycn5uWVtkbziSob5HjR3Yoe1yT6E\nEOJM19oHVC8HfmiqS0Zr/YbWeqTWemR0dHQr79rIK6mgPwegU6K5EIcQQnRALQn3DCDB5X68c1lj\nbsCNXTIAoYX7GFa2DhLPdWcxhBDCrVoS7huB3kqpJKWUHybAF9ZfSSkVDkwEvmjdIrac1ppbKuZj\ns/jBRX9wVzGEEMLtmg13rbUNeABYAuwBPtJa71JK3auUutdl1RnAUq11SdsUtXmFBSeYrNaTGjcD\ngqPcVQwhhHC7Fs0KqbVeBCyqt2xuvftvA2+3VsFORcmhnwhXmqJu49xZDCGEcDuvOkPVlrEVACWj\nZIQQHZxXhbvv0Q0c050Ii250pKYQQnQY3hPulSVEZ65imX0EUSEyA6QQomPznnA/tA4feznfOEbR\nKcjP3aURQgi38p5wzzazIRzx642fj/dUSwghToX3XEM1ew95qjPRXbq5uyRCCOF2XtPELT+6k122\nOC4bLOEuhBDeEe4OOz7H97JXxzP+LDl5SQghvCPcTxzEx15OGt1Jigp2d2mEEMLtvCLc03ZtBKA0\nog++Vq+okhBCnBaPT8LC8io+XbICgKC4Ac2sLYQQHYPHh3t+SRWxKpfjOoRJg3q6uzhCCHFG8Phw\nL6qoops6TqaOZFLftrkAiBBCeBrPD/dyG7Eql67dexPga3V3cYQQ4ozgJeGehw6LdXdRhBDijOHx\n4V5enE+4KsUSkdD8ykII0UF4fLg7CtIB8JFwF0KIGh4f7qrQXKvbP6q7m0sihBBnDo8Pd9/iowD4\nd5ZwF0KIah4f7v6lmdixQKhMGCaEENU8PtyDyo6RpzqB1XtmLxZCiNPl0eHucGgcxdmU+ka6uyhC\nCHFG8dhw11oz5aU1+NuKCAqXcBdCCFceG+67jhaSklVEV79yIqNi3F0cIYQ4o3hsR/XqvTkAdPMr\nxxIU4ebSCCHEmcVjW+6FZVUE+lqwVBRAgIS7EEK4alG4K6UmK6VSlFJpSqlZTawzSSm1VSm1Sym1\nunWL2VBZlZ0IHxvYKyFQwl0IIVw12y2jlLICc4CLgXRgo1JqodZ6t8s6EcBrwGSt9WGlVJe2KnC1\nsko70b5lUAkEhLf17oQQwqO0pOU+GkjTWu/XWlcC84Hp9da5CfhUa30YQGud3brFbKisyk6UT7m5\nI90yQghRR0vCPQ444nI/3bnMVR+gk1JqlVJqs1LqttYqYFPKq+xEWkvNHemWEUKIOlprtIwPMAK4\nEAgE1iml1mut97qupJSaCcwE6N799OaCKauyM7r6Myes/meNEEJ0bC1puWcArvPpxjuXuUoHlmit\nS7TWucB3wJD6G9Jav6G1Hqm1HhkdfXqXxCurtDOhcg1E9TE/QggharQk3DcCvZVSSUopP+AGYGG9\ndb4AJiilfJRSQcA5wJ7WLWpdurKUPhU7od80UKotdyWEEB6n2W4ZrbVNKfUAsASwAvO01ruUUvc6\nH5+rtd6jlPoG2A44gH9rrXe2ZcETy5PxwQ7dx7blboQQwiO1qM9da70IWFRv2dx6958Dnmu9op1c\n76pkcyN+VHvtUgghPIbHnqEabz9CoW8UBHV2d1GEEOKM47HhHquPkR8Q7+5iCCHEGckjw93h0CSQ\nRVGghLsQQjTGI8O9vLSQGJVPSbBcN1UIIRrjkeFeWWBmN6gKknnchRCiMZ4Z7uUlACjfQDeXRAgh\nzkweGe6OKjNhmPYJcHNJhBDizOSR4a6rysxvCXchhGiUR4c7Eu5CCNEojwx3bBUAOCTchRCiUR4Z\n7rq6z90q4S6EEI3xyHDHGe7K19/NBRFCiDOTZ4a7rfqAqgyFFEKIxnhmuDtb7vhIy10IIRrjmeFu\nd4a7nMQkhBCN8shwV9V97jJaRgghGuWR4Y69ggrtg8VidXdJhBDijOSR4a5s5VTgh8UjSy+EEG3P\nI+NR2cooxw+rXBhbCCEa5aHhXkGF9sVqkXAXQojGeGa428spxw+LhLsQQjTKQ8O9gnJ8pVtGCCGa\n4JHhbrFXUoWPdMsIIUQTPDLccdixY0Ea7kII0TjPDHccOLBIy10IIZrgmeHucODQFulzF0KIJnhm\nuOPAgZLRMkII0YQWhbtSarJSKkUplaaUmtXI45OUUgVKqa3On2dav6guHCbcpeUuhBCN82luBaWU\nFZgDXAykAxuVUgu11rvrrbpGa31ZG5SxIS197kIIcTItabmPBtK01vu11pXAfGB62xarGdq03KXh\nLoQQjWtJuMcBR1zupzuX1TdOKbVdKbVYKTWwsQ0ppWYqpTYppTbl5OScQnGd29F2abkLIcRJtNYB\n1S1Ad631YOAV4PPGVtJav6G1Hqm1HhkdHX3qe9PahLs03YUQolEtCfcMIMHlfrxzWQ2tdaHWuth5\nexHgq5SKarVS1qdltIwQQpxMS8J9I9BbKZWklPIDbgAWuq6glOqqlGlGK6VGO7eb19qFrdmftsto\nGSGEOIlmR8torW1KqQeAJYAVmKe13qWUutf5+FzgGuA+pZQNKANu0FrrNiu1dmDHIi13IYRoQrPh\nDjVdLYvqLZvrcvtV4NXWLVrTFBqNkgOqQgjRBI88Q7V6tIxkuxBCNM4jw72mW0b63IUQolEeGu7S\nLSOEECfjkeGusOPQMlpGCCGa4pnh7jyJSUbLCCFE4zw03B1o5ZFFF0KIduGhCelAI612IYRoikeG\nu0Va7kIIcVKemZAS7kIIcVIemZDmDFWPLLoQQrQLj0xIpe3SchdCiJPwyIRUaAl3IYQ4CY9MSKUd\neGjRhRCiXXhkQlpwoOXsVCGEaJJHhrs5icnq7mIIIcQZyzPDHY2HFl0IIdqF5yWk1s5uGc8ruhBC\ntBfPS8jqq/dZPK/oQgjRXjwvIbXD/PLAogshRHvxvIR0hjsyWkYIIZrkgeFuN7+lz10IIZrkeQlZ\n3S0jQyGFEKJJHhvu0nIXQoimeV5COqRbRgghmuN5CVnTLeN5RRdCiPbieQnpHOdutUqfuxBCNMWn\nJSsppSYDLwFW4N9a69lNrDcKWAfcoLVe0GqldOUcLWOxSLgL0RFVVVWRnp5OeXm5u4vSpgICAoiP\nj8fX1/eUnt9suCulrMAc4GIgHdiolFqotd7dyHp/A5aeUklaytktoyTcheiQ0tPTCQ0NJTExEeWl\n57torcnLyyM9PZ2kpKRT2kZLumVGA2la6/1a60pgPjC9kfV+DXwCZJ9SSVrKGe7SLSNEx1ReXk5k\nZKTXBjuAUorIyMjT+nbSknCPA4643E93LnMtSBwwA/jXKZekpRzV3TKed7hACNE6vDnYq51uHVsr\nIV8EntS6ehB645RSM5VSm5RSm3Jyck5tT85dWKwtOlwghBCtKj8/n9dee+1nP2/q1Knk5+e3QYka\n15JwzwASXO7HO5e5GgnMV0odBK4BXlNKXVl/Q1rrN7TWI7XWI6Ojo0+txDXdMtJyF0K0v6bC3Waz\nnfR5ixYtIiIioq2K1UBLmr8bgd5KqSRMqN8A3OS6gta6psdfKfU28JXW+vNWLKfLzpzhbpGWuxCi\n/c2aNYt9+/YxdOhQfH19CQgIoFOnTiQnJ7N3716uvPJKjhw5Qnl5OQ899BAzZ84EIDExkU2bNlFc\nXMyUKVOYMGECa9euJS4uji+++ILAwMBWLWezCam1timlHgCWYIZCztNa71JK3et8fG6rlqjZAlV3\ny0jLXYiO7tkvd7H7aGGrbnNAbBi/v3xgk4/Pnj2bnTt3snXrVlatWsW0adPYuXNnzaiWefPm0blz\nZ8rKyhg1ahRXX301kZGRdbaRmprKBx98wJtvvsl1113HJ598wi233NKq9WhR81drvQhYVG9Zo6Gu\ntb7j9It1krI47CjAKn3uQogzwOjRo+sMV3z55Zf57LPPADhy5AipqakNwj0pKYmhQ4cCMGLECA4e\nPNjq5fK4hLTZ7fgiQyGFEJy0hd1egoODa26vWrWK5cuXs27dOoKCgpg0aVKjwxn9/f1rblutVsrK\nylq9XB7Xt1HlPGjhI+EuhHCD0NBQioqKGn2soKCATp06ERQURHJyMuvXr2/n0tXyuJZ7VZUJd2m5\nCyHcITIykvHjx3P22WcTGBhITExMzWOTJ09m7ty59O/fn759+zJmzBi3ldPjwt1md57EJOEuhHCT\n999/v9Hl/v7+LF68uNHHqvvVo6Ki2LlzZ83yxx57rNXLBx7YLWOTbhkhhGiWx4V7pTPcrT4e96VD\nCCHajceFu81mumV8peUuhBBN8sBwrz6gKi13IYRoiueFu126ZYQQojmeF+7Obhk5oCqEEE3zuHC3\nV4+W8ZFwF0K0v1Od8hfgxRdfpLS0tJVL1DiPC/cq5zh3Xwl3IYQbeEq4e1zHtd1e3S1zaheNFUKI\n0+E65e/FF19Mly5d+Oijj6ioqGDGjBk8++yzlJSUcN1115Geno7dbud3v/sdWVlZHD16lPPPP5+o\nqChWrlzZpuX0uHAf3SMC1kJcp+DmVxZCeLfFs+DYjtbdZtdBMGV2kw+7Tvm7dOlSFixYwIYNG9Ba\nc8UVV/Ddd9+Rk5NDbGwsX3/9NWDmnAkPD+eFF15g5cqVREVFtW6ZG+Fx3TIWZD53IcSZYenSpSxd\nupRhw4YxfPhwkpOTSU1NZdCgQSxbtownn3ySNWvWEB4e3u5l87iWe/XFOlDS5y5Eh3eSFnZ70Frz\n1FNPcc899zR4bMuWLSxatIinn36aCy+8kGeeeaZdy+Z5zd+acPe8ogshPJ/rlL+XXnop8+bNo7i4\nGICMjAyys7M5evQoQUFB3HLLLTz++ONs2bKlwXPbmge23M0BVQl3IYQ7uE75O2XKFG666SbGjh0L\nQEhICO+++y5paWk8/vjjWCwWfH19+de//gXAzJkzmTx5MrGxsW1+QFVprdt0B00ZOXKk3rRp089/\n4q7P4OM74FfroUv/Vi+XEOLMtmfPHvr37xj/+43VVSm1WWs9srnnel7zNzQWBlwJ/mHuLokQQpyx\nPK9bpvs55kcIIUSTPK/lLoQQolkS7kIIj+OuY4Xt6XTrKOEuhPAoAQEB5OXleXXAa63Jy8sjICDg\nlLfheX3uQogOLT4+nvT0dHJyctxdlDYVEBBAfHz8KT9fwl0I4VF8fX1JSkpydzHOeNItI4QQXkjC\nXQghvJCEuxBCeCG3TT+glMoBDp3i06OA3FYsjieQOncMUueO4XTq3ENrHd3cSm4L99OhlNrUkrkV\nvInUuWOQOncM7VFn6ZYRQggvJOEuhBBeyFPD/Q13F8ANpM4dg9S5Y2jzOntkn7sQQoiT89SWuxBC\niJPwuHBXSk1WSqUopdKUUrPcXZ7WopSap5TKVkrtdFnWWSm1TCmV6vzdyeWxp5yvQYpS6lL3lPr0\nKKUSlFIrlVK7lVK7lFIPOZd7bb2VUgFKqQ1KqW3OOj/rXO61dQZQSlmVUj8ppb5y3vfq+gIopQ4q\npXYopbYqpTY5l7VfvbXWHvMDWIF9QE/AD9gGDHB3uVqpbucBw4GdLsv+Dsxy3p4F/M15e4Cz7v5A\nkvM1sbq7DqdQ527AcOftUGCvs25eW29AASHO277Aj8AYb66zsx6PAu8DXznve3V9nXU5CETVW9Zu\n9fa0lvtoIE1rvV9rXQnMB6a7uUytQmv9HXC83uLpwH+dt/8LXOmyfL7WukJrfQBIw7w2HkVrnam1\n3uK8XQTsAeLw4npro9h519f5o/HiOiul4oFpwL9dFnttfZvRbvX2tHCPA4643E93LvNWMVrrTOft\nY0CM87bXvQ5KqURgGKYl69X1dnZRbAWygWVaa2+v84vAE4DDZZk317eaBpYrpTYrpWY6l7VbvWXK\nXw+htdZKKa8c2qSUCgE+AR7WWhcqpWoe88Z6a63twFClVATwmVLq7HqPe02dlVKXAdla681KqUmN\nreNN9a1ngtY6QynVBVimlEp2fbCt6+1pLfcMIMHlfrxzmbfKUkp1A3D+znYu95rXQSnliwn297TW\nnzoXe329AbTW+cBKYDLeW+fxwBVKqYOYbtQLlFLv4r31raG1znD+zgY+w3SztFu9PS3cNwK9lVJJ\nSik/4AZgoZvL1JYWArc7b98OfOGy/AallL9SKgnoDWxwQ/lOizJN9P8Ae7TWL7g85LX1VkpFO1vs\nKKUCgYuBZLy0zlrrp7TW8VrrRMz/6wqt9S14aX2rKaWClVKh1beBS4CdtGe93X1E+RSOQE/FjKrY\nB/zW3eVpxXp9AGQCVZj+truASOBbIBVYDnR2Wf+3ztcgBZji7vKfYp0nYPoltwNbnT9TvbnewGDg\nJ2eddwLPOJd7bZ1d6jGJ2tEyXl1fzIi+bc6fXdVZ1Z71ljNUhRDCC3lat4wQQogWkHAXQggvJOEu\nhBBeSMJdCCG8kIS7EEJ4IQl3IYTwQhLuQgjhhSTchRDCC/1//kopcviJUUAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12f0b1710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot learning curves of model accuracy\n",
    "pyplot.plot(history.history['acc'], label='train')\n",
    "pyplot.plot(history.history['val_acc'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:aind-dog]",
   "language": "python",
   "name": "conda-env-aind-dog-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
