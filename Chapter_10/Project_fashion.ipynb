{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Visual Embedding Chapter\n",
    "## Fashion - select visually similar apparels \n",
    "## This notebook is for illustrating how to train a model using triplet-metric-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingredients:\n",
    "### Machine configurations\n",
    "* A machine with a GPU supporting CUDA 9.2+\n",
    "* Tensorflow 1.4\n",
    "* Python 3\n",
    "An alternative to build above environment is to directly use a docker with relevant infomation. You can download from here - [TF Docker](https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel_19.07.html#rel_19.07).\n",
    "\n",
    "### Dataset\n",
    "* Download fasihon dataset -  [In-Shop Clothes Retrieval Benchmark](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset\n",
    "In order to be able to use the training code, we will generate list of images (train, test, query) into csv file - then we will be able to feed in to the training code seamlessly.\n",
    "\n",
    "*REQUIREMENT* The dataset (image_location) and (class_name) needs to be put in a csv format. A sample two row of a csv file would look like:\n",
    "\n",
    "`train_images, 12`\n",
    "\n",
    "`train_images, 33`\n",
    "\n",
    "So there will be a need of three output files, one for each of - `train_set`, `test_set` or `query_set`.\n",
    "\n",
    "The folder `preprocessors/` hasn an utility function for obtaining the csv from the downloaded dataset. \n",
    "Fashion - `Fashion_convert2defense_triplet_format.py` . Please replace the variable `split_file` appropriately from the downloaded dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone the infrastructure code\n",
    "\n",
    "Location - [Github](https://github.com/VisualComputingInstitute/triplet-reid/tree/sampling)\n",
    "*Note* that we are cloning the `sampling` branch which has three sampling (mining variants):\n",
    "1. Batch All (BA)\n",
    "2. Batch Hard (BH)\n",
    "3. Batch Sample (BS)\n",
    "\n",
    "In order to freeze the repo status used for the book results, you can refer to the fork here - [Link](https://github.com/ratnesh1729/triplet-reid/tree/sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Hyper-parameters \n",
    "\n",
    "We have the following parameters need to be set for training:\n",
    "1. Network model: Choose a base architecture: Above code supplies options for Resnet-50, Resnet-101, MobileNet_v1. \n",
    "2. Pre-trained: Whether we need pre-trained model of above - Generally the answer is `yes`. \n",
    "3. Data augmentation: We could choose to randomly flip and crop images. \n",
    "4. Embedding dimension: Any feasible number. \n",
    "5. Batch Size: Parameterized by `P, K` , corresponding to `P` number of classes to choose and `K` samples from each class. So total batch size is `P*K`.\n",
    "6. Crop initial images - If crop augmentation is used, we would need to supply initial crop width and height.\n",
    "7. Network input size - Imagenet is generally trained with `224x224` and apparels are generally isotropic in dimensions, so we should stick to that. (Notice for training for person re-id, this assumption is not generally applicable as a person's `height > width`).\n",
    "8. Mining variant: BS, BH, BA\n",
    "9. Learning rate: Generally a low setting if we're utilizing pre-trained network.\n",
    "10. Learning rate decay: Number of iterations before dropping the learning rate. \n",
    "11. Metric to compare: Choices are `square_euclidean` or `euclidean`. In practicle `euclidean` seems to work better (also on this dataset).\n",
    "12. Margin: We will use the `softplus` option. Other possiblity would be to use `hard margin`, by supplying a float parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training configuration\n",
    "The following bash script could be directly use for training on fashion dataset.\n",
    "\n",
    "*The paths should be set appropriately* - Train csv file, Output location, Input pre--trained model, Image file location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/sh\n",
    "####\n",
    "#### This file calls train.py with all hyperparameters as for the triplet metric learning experiment on In--Store Shopping Retrieval Project.\n",
    "\n",
    "\n",
    "#### Shift the arguments so that we can just forward the remainder.\n",
    "IMAGE_ROOT=/datasets/fashion/\n",
    "EXP_ROOT=/training_output/\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python train.py \\\n",
    "    --train_set /fashion/codes_preprocessing/in_shop_defense_triplet_loss_format_TRAIN.csv \\\n",
    "    --model_name mobilenet_v1_1_224 \\\n",
    "    --image_root $IMAGE_ROOT \\\n",
    "    --initial_checkpoint /home/Downloads/mobilenet_v1_1.0_224/mobilenet_v1_1.0_224.ckpt\\\n",
    "    --experiment_root $EXP_ROOT \\\n",
    "    --flip_augment \\\n",
    "    --embedding_dim 128 \\\n",
    "    --batch_p 18 \\\n",
    "    --batch_k 4 \\\n",
    "    --net_input_height 224 --net_input_width 224 \\\n",
    "    --margin soft \\\n",
    "    --metric euclidean \\\n",
    "    --loss batch_all \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --train_iterations 100000 \\\n",
    "    --head_name direct \\\n",
    "    --decay_start_iteration 25000\\\n",
    "    \"$@\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the trainig progress:\n",
    "As demonstrated by the authors of above could, we could use `tensorboard` to visualize the output. \n",
    "Here is an output of healthy run - healthy-run-sample\n",
    "\n",
    "\n",
    "![title](healthy-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing a trained model, Qualitatively\n",
    "\n",
    "We could visualize the embeddings by showing them as following. First image in each row is the `query` image, while rest are `top-k` retrievals.\n",
    "\n",
    "![title](viz/sample_visuals/000184.png)\n",
    "![title](viz/sample_visuals/004032.png)\n",
    "![title](viz/sample_visuals/007947.png)\n",
    "\n",
    "##### Utility for above code\n",
    "* `python viz/viz_retrievls.py ---h` . You would need Spotify's approximate nearest neighbor library - `pip install annoy`\n",
    "* Sample command: \n",
    "`python viz_retrievals.py --img /datasets/fashion/ --query_csv /datasets/fashion/codes_preprocessing/in_shop_defense_triplet_loss_format_QUERY.csv --query_h5 trained_model_folder/in_shop_defense_triplet_loss_format_QUERY_embeddings.h5 --gallery_csv /datasets/fashion/codes_preprocessing/in_shop_defense_triplet_loss_format_GALLERY.csv --gallery_h5 trained_model_folder/in_shop_defense_triplet_loss_format_GALLERY_embeddings.h5 --k 5 --output top_5_viz_results`\n",
    "\n",
    "*Notice* that the above code uses Soptify's Annoy library (Approximate nearest neighbors) for efficient retrievals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing a trained model, Quantitatively\n",
    "\n",
    "1. Step 1: Generate embeddings (stored in `.h5`) file using `embed.py` for both `Query` and `Test` set.\n",
    "\n",
    "A sample for query - `python embed.py --experiment_root /train_output --dataset in_shop_defense_triplet_loss_format_QUERY.csv --image_root /datasets/fashion/ --checkpoint checkpoint-100000`\n",
    "\n",
    "A sample for test - `python embed.py --experiment_root /train_output --dataset in_shop_defense_triplet_loss_format_GALLERY.csv --image_root /datasets/fashion/ --checkpoint checkpoint-100000`\n",
    "*Note* By default the `.h5` files are stored in the `training-output` directory.\n",
    "\n",
    "2. Step 2: Evaluate these embeddings using `evaluate.py`. This will geneerate the `top-k` and `mAP` on the terminal.\n",
    "\n",
    "A sample - \n",
    "` python evaluate.py --excluder diagonal --query_dataset in_shop_defense_triplet_loss_format_QUERY.csv --query_embeddings ./train_output/in_shop_defense_triplet_loss_format_QUERY_embeddings.h5 --gallery_dataset in_shop_defense_triplet_loss_format_GALLERY.csv --gallery_embeddings /train_output/in_shop_defense_triplet_loss_format_GALLERY_embeddings.h5 --metric euclidean`\n",
    "\n",
    "A sample output would look like: \n",
    "`mAP: 72.40% | top-1: 86.40% top-2: 91.22% | top-5: 95.43% | top-10:96.85% | top-20: 97.83%`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_1_9_cuda_9_2",
   "language": "python",
   "name": "tf_1_9_cuda_9_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
